# 소프트맥스 회귀

- 소프트맥스 회귀(Sotfmax Regression)
  - 앞에서 살펴본 로지스틱 회귀는 2개의 선택지 중에서 1개를 고르는 이진 분류를 푸는 방식이었다.
    - 로지스틱 회귀에서 사용한 시그모이드 함수는 입력된 데이터에 대해서 0과 1 사이의 값을 출력하여 해당 값이 둘 중 하나에 속할 확률로 해석할 수 있도록 만들어줬다.
    - 예를 들어 0이 정상 메일, 1이 스팸 메일이라고 정의해놓는다면 시그모이드 함수의 0과 1 사이의 출력값을 스팸 메일일 확률로 해석할 수 있었다.
    - 확률값이 0.5를 넘으면 1에 가까우니 스팸 메일로 판단하면 되고, 그 반대라면 정상 메일로 판단하면 된다.
  - 소프트맥스 회귀는 3개 이상의 선택지 중에서 1개를 고르는 다중 클래스 분류 문제를 푸는 방식이다.
    - 이진 분류가 두 개의 선택지 중 하나를 고르는 문제였다면, 세 개 이상의 선택지 중 하나를 고르는 문제를 다중 클래스 분류라고 한다.
    - 전체 선택지에 걸친 확률의 합계가 1이 되도록 하여 전체 선택지 중 가장 확률이 높은 선택지를 선택하도록 하는 소프트맥스 함수를 사용한다.



- 소프트맥스 함수(Softmax function)

  - 선택해야 하는 선택지의 총 개수를 k라고 할 때, k차원의 벡터를 입력 받아 각 클래스에 대한 확률을 추정한다.
    - k차원의 벡터에서 i번째 원소를 $z_i$, i번째 클래스가 정답일 확률을 $p_i$로 나타낼 때, 소프트맥스 함수는 $p_i$를 아래와 같이 정의한다.

  $$
  p_i={e^{z_i}\over \sum_{j=1}^k}for\ \ i=1,2,...k
  $$

  - 예를 들어 꽃에 대한 데이터가 있을 때 그 꽃이 a, b, c 중 어떤 꽃일지 예측하는 경우를 생각해보자.
    - 이 경우 k는 3이므로 3차원 벡터 $z=[z_1\ z_2\ z_3]$의 입력을 받으면 소프트맥스 함수는 아래와 같은 출력을 반환한다.
    - $p_1,p_2,p_3$ 각각은 1~3번 클래스(setosa, versicolor, virginica)가 정답일 확률을 나타내며, 각각 0과 1사이의 값으로 총합은 1이 된다.

  $$
  softmax(z)=[{e^{z_1} \over \sum_{j=1}^3e^{z_j}}{e^{z_2} \over \sum_{j=1}^3e^{z_j}}{e^{z_3} \over \sum_{j=1}^3e^{z_j}}]=[p_1,p_2,p_3]=\hat y = 예측값
  $$

  - 이 때, 꽃에 대한 데이터는 꽃받침 길이와 넓이, 꽃의 길이, 꽃잎 넓이가 있다고 하자.
    - 이 4가지 요소가 독립변수 $x_1,x_2,x_3,x_4$가 된다.
    - 이는 모델이 4차원 벡터를 입력으로 받음을 의미한다.
    - 그런데 소프트맥스 함수의 입력으로 사용되는 벡터는 차원이 분류하고자 하는 클래스의 개수(k, 예시의 경우 3)가 되어야 하므로 어떤 가중치 연산을 통해 3차원 벡터로 변환되어야한다.
  - 샘플 데이터를 소프트맥스 함수의 입력 벡터로 차원을 축소하는 방법
    - 샘플 데이터(독립 변수 벡터)를 소프트맥스 함수의 입력 벡터로 차원을 축소하는 방법은 간단하다.
    - 소프트맥스 함수의 입력 벡터 $z$의 차원수만큼 결과값이 나오도록 가중치 곱을 진행한다.
    - 경우의 수는 4*3으로 총 12개이며 전부 다른 가중치를 가지고, 학습 과정에서 점차적으로 오차를 최소화하는 가중치로 값이 변경된다.
  - 오차를 계산하는 방법
    - 소프트맥스 함수의 출력은 분류하고자하는 클래스의 개수만큼 차원을 가지는 벡터로 각 원소는 0과 1사이의 값을 가지며, 각각은 특정 클래스가 정답일 확률을 나타낸다.
    - 그렇다면 이 예측값과 비교를 할 수 있는 실제값의 표현 방법이 있어야하는데, 소프트맥스 회귀에서는 실제값을 원-핫 벡터로 표현한다.
    - a,b,c의 원-핫 벡터를 각기 [1,0,0], [0,1,0], [0,0,1]이라고 하자.
    - 이 때, 현재 풀고 있는 샘플 데이터의 실제 값이 b라면 b의 원-핫 벡터는 [0,1,0]이다.
    - 이 경우 예측값과 실제값의 오차가 0이 되는 경우는 소프트맥스 함수의 결과가 [0, 1, 0]이 되는 경우이다.
    - 이 두 벡터의 오차를 계산하기 위해서 소프트맥스 회귀는 비용 함수로 크로스 엔트로피 함수를 사용한다.
  - 선형 회귀나 로지스틱 회귀와 마찬가지로 소프트맥스 회귀 역시 오차로부터 가중치를 업데이트한다.
    - 더 정확히는 선형회귀나 로지스틱 회귀와 마찬가지로 편향 또한 업데이트 대상이 되는 매개 변수이다.



- 원-핫 벡터를 사용하는 이유

  - 꼭 실제값을 원-핫 벡터로 표현해야만 다중 클래스 분류 문제를 풀 수 있는 것은 아니다.

    - 다만 대부분의 다중 클래스 분류 문제가 각 클래스 간의 관계가 균등하다는 점에서 원-핫 벡터는 이러한 점을 표현할 수 있는 적절한 표현 방법이다.

  - 다수의 클래스를 분류하는 문제에서는 이진 분류처럼 2개의 숫자 레이블이 아니라 클래스의 개수만큼 숫자 레이블이 필요하다.

    - 이때 직관적으로 생각해볼 수 있는 레이블링 방법은 분류해야 할 클래스 전체에 정수 인코딩을 하는 것이다.
    - 예를 들어 분류해야 할 레이블이 red, green, blue와 같이 3개라면 각각 0,1,2로 레이블한다.
    - 또한 분류해야 할 클래스가 north, east, west, south 네 개이고, 인덱스를 숫자 1부터 시작하고 싶다면 1,2,3,4로 레이블 해볼 수 있다.

  - 그런데 일반적인 다중 클래스 분류 문제에서 레이블링 방법으로는 위와 같은 정수 인코딩이 아니라 원-핫 인코딩을 사용하는 것이 보다 클래스의 성질을 잘 표현했다고 할 수 있다.

    - 예를 들어 a, b, c라는 3개의 클래스가 있는 문제가 있다고 가정해보자.
    - 레이블은 정수 인코딩을 사용하여 각각 1,2,3을 부여하였다.
    - 손실 함수로 평균 제곱 오차를 사용하면 정수 인코딩이 오해를 불러일으킬 수 있다.
    - 아래는 앞서 살펴본 평균 제곱 오차의 식을 가져온 것이다.

    $$
    {1 \over n}\sum_{i}^n(y_i-\hat y_i)^2
    $$

    - 직관적인 오차 크기 비교를 위해 평균을 구하는 수식은 제외하고 제곱 오차로만 판단하면 실제 값이 b일때 예측값이 a였다면 제곱 오차는 $(2-1)^2=1$이 된다.
    - 실제값이 c일때 예측값이 a였다면 제곱 오차는 $(3-1)^2=4$가 된다.
    - 즉 b와 c 사이의 오차보다 c와 a 사이의 오차가 더 크다.
    - 이는 기계에게 b와 c가 a와 c보다 더 가깝다는 정보를 주는 것과 같다.

  - 원-핫 벡터의 무작위성

    - 일반적인 분류 문제에서 각 클래스는 순서의 의미를 갖고 있지 않으므로 각 클래스간 오차는 균등한 것이 맞다.
    - 정수 인코딩과 달리 원-핫 인코딩은 분류 문제의 모든 클래스 간의 관계를 균등하게 분배한다.
    - 아래는 세 개의 카테고리에 대해서 원-핫 인코딩을 통해서 레이블을 했을 때 각 클래스 간의 제곱 오차가 균등합을 보여준다.
    - $((1,0,0)-(0,1,0))^2=2$, $((1,0,0)-(0,0,1))^2=2$
    - 다르게 표현하면 모든 클래스에 대해서 원-핫 인코딩을 통해 얻은 원-핫 벡터들은 모든 쌍에 대해 유클리드 거리를 구해도 전부 유클리드 거리가 동일하다.
    - 원-핫 벡터는 이처럼 각 클래스의 표현 방법이 무작위성을 가진다는 점을 표현할 수 있다.



- 비용 함수(Cost function)

  - 크로스 엔트로피 함수
    - 소프트맥스 회귀에서는 비용 함수로 크로스 엔트로피 함수를 사용한다.
    - 아래 식에서 $y$는 실제값, $k$는 클래스의 개수, $y_j$는 실제 원-핫 벡터의 $j$번째 인덱스, $p_j$는 샘플 데이터가 $j$번째 클래스일 확률을 나타낸다.
    - $p_j$는 표기에 따라서 $\hat y_j$로 표현하기도 한다.
    - 원-핫 벡터에서 1인 값은 한 개만 존재하므로, 결국 $p_j$는 원-핫 벡터의 j번째 원소가 1일 확률(즉 j번째 클래스일 확률)을 의미한다.
  
  $$
  cost = -\sum_{j=1}^ky_jlog(p_j)
  $$
  
  - 크로스 엔트로피 함수를 비용 함수로 사용하는 이유
    - $c$가 실제값 원-핫 벡터에서 1을 가진 원소의 인덱스라고 한다면, $p_c=1$은 $\hat y$가 $y$를 정확하게 예측한 경우가 된다.
    - 이를 식에 대입해보면 $-1log(1)=0$이 되기 때문에, 결과적으로  $\hat y$가 $y$를 정확하게 예측한 경우의 크로스 엔트로피 함수의 값은 0이 된다.
    - 즉 , 크로스 엔트로피 함수의 출력값을 최소화 하는 방향으로 학습해야한다.
  - 이를 n개의 전체 데이터에 대한 평균을 구한다고하면 최종 비용 함수는 아래와 같다.
  
  $$
  cost = -{1 \over n}\sum_{i=1}^n \sum_{j=1}^k y_j^{(i)}log(p_j^{(i)})
  $$
  
  - 이진 분류에서의 크로스 엔트로피 함수
  
    - 로지스틱 회귀에서 봤던 크로스 엔트로피 함수식과 달라보이지만 본질적으로 동일한 함수식이다.
    - 로지스틱 회귀의 크로스 엔트로피 함수식으로부터 소프트맥스 회귀의 크로스 엔트로피 함수식을 도출하면 아래와 같다.
  
    $$
    cost = -(ylogH(X)+(1-y)log(1-H(X)))
    $$
  
    - 위 식은 앞서 로지스틱 회귀에서 봤던 크로스 엔트로피의 함수식을 보여주는데, 위 식에서 $y$를 $y_1$, $1-y$를 $y_2$로 치환하고 $H(X)$를 $p_1$, $1-H(X)$를 $p_2$로 치환하면 아래의 수식을 얻을 수 있다.
  
    $$
    -(y_1log(p_1)+y_2\ log(p_2))
    $$
  
    - 위 식은 아래와 같이 표현할 수 있다.
  
    $$
    -(\sum_{i=1}^2y_i\ log\ p_i)
    $$
  
    - 소프트맥스 회귀에서는 k의 값이 고정된 값이 아니므로 2를 k로 변경해야한다.
  
    $$
    -(\sum_{i=1}^ky_i\ log\ p_i)
    $$
  
    
  
    - 위 식은 결과적으로 소프트맥스 회귀의 식과 동일하며, 역으로 소프트맥스 회귀에서 로지스틱 회귀의 크로스 엔트로피 함수식을 얻는 것은 k를 2로하고 를 $y_1$, $y_2$를 $y$와 $y-1$로 치환하고, $p_1$, $p_2$를 $H(X)$, $1-H(X)$로 치환하면 된다.
    - 정리하면 소프트맥스 함수의 최종 비용 함수에서 k가 2라고 가정하면 결국 로지스틱 회귀의 비용 함수와 같다.
  
    $$
    cost = -{1 \over n}\sum_{i=1}^n \sum_{j=1}^k y_j^{(i)}log(p_j^{(i)})=-{1 \over n} \sum_{i=1}^n[y^{(i)}log(p^{(i)})+(i-y^{(i)})log(1-p^{(i)})]
    $$



- 소프트맥스 회귀 실습

  > 아이리스 품종 데이터로 실습을 진해한다.
  >
  > https://www.kaggle.com/datasets/saurabh00007/iriscsv에서 데이터를 다운 받아야한다.

  - 필요한 패키지 들을 설치한다.

  ```bash
  $ pip install pandas seaborn scikit-learn tensorflow
  ```

  - 아이리스 품종 데이터에 대한 이해
    - 아래와 같이 다운 받은 데이터 파일에서 5개의 샘플을 확인한다.
    - 데이터는 6개의 열로 구성된 총 150개의 샘플로 구성되어 있다.
    - 특성에 해당하는 `SepalLengthCm`, `SepalWidthCm`, `PetalLengthCm`, `PetalWidthCm` 4개의 열이 있다.
    - 마지막 열  `Species`는 해당 샘플이 어떤 품종인지를 의미하며, Iris‑setosa, Iris‑versicolor, Iris‑virginica 라는 3 개의 품종으로 구성되고, 이 실습에서 예측해야 하는 레이블에 해당한다.
    - 즉 이 실습은 주어진 샘플 데이터의 4개의 특성으로부터 3개의 품종 중 어떤 품종인지를 예측하는 것이다.

  ```python
  import pandas as pd
  import seaborn as sns
  import matplotlib.pyplot as plt
  from sklearn.model_selection import train_test_split
  from tensorflow.keras.utils import to_categorical
  
  
  data = pd.read_csv("Iris.csv", encoding="latin1")
  print(data[:5])
  
  """
     Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species
  0   1            5.1           3.5            1.4           0.2  Iris-setosa
  1   2            4.9           3.0            1.4           0.2  Iris-setosa
  2   3            4.7           3.2            1.3           0.2  Iris-setosa
  3   4            4.6           3.1            1.5           0.2  Iris-setosa
  4   5            5.0           3.6            1.4           0.2  Iris-setosa
  """
  ```

  - 3개 품종이 4개의 특성에 대해 어떤 분포를 가지고 있는지 확인하기 위해 아래와 같이 시각화가 가능하다.
    - 4개의 특성 `SepalLengthCm`, `SepalWidthCm`, `PetalLengthCm`, `PetalWidthCm`에 대하 모든 쌍의 조합인 16개의 경우에 대해서 산점도를 그린다.
    - 만약 동일한 특성의 쌍일 경우에는 히스토그램으로 나타낸다. 

  ```python
  sns.set(style="ticks", color_codes=True)
  g = sns.pairplot(data, hue="Species", palette="husl")
  g.savefig("iris.png")
  ```

  - 아래와 같이 종과 `SepalLengthCm` 특성에 대한 연관 관계를 출력할 수 도 있다.
    - 아래 코드 실행시, 위에서 시각한 코드와 별도로 실행해야하며, 그렇지 않을 경우 이전 결과에 덮어씌워진다.

  ```python
  ax = sns.barplot(x='Species', y='SepalWidthCm', data=data)
  ax.figure.savefig("species_sepal_width_cm.png")
  ```

  - 150개의 샘플 데이터에 각 품종이 몇 개씩 있는지 확인한다.
    - 모두 동일하게 50개씩 있는 것을 확인할 수 있다.
    - 즉 각 레이블에 대한 분포가 균일하다. 

  ```python
  print(data['Species'].value_counts())
  
  """
  Species
  Iris-setosa        50
  Iris-versicolor    50
  Iris-virginica     50
  Name: count, dtype: int64
  """
  ```

  - 레이블에 해당하는 `Species` 열에 대해 전부 수치화를 진행한다.
    - 우선 원-핫 인코딩을 수행하기 전 0, 1, 2로 정수 인코딩을 수행한다.
    - 그 후 여전히 동일한 분포를 보이는지 확인한다.

  ```python
  # 정수 인코딩 수행
  data['Species'] = data['Species'].replace(['Iris-virginica','Iris-setosa','Iris-versicolor'],[0,1,2])
  
  # 분포 재확인
  print(data['Species'].value_counts())
  
  """
  Species
  1    50
  2    50
  0    50
  Name: count, dtype: int64
  """
  ```

  - 특성과 품종을 각각 독립 변수와 종속 변수 데이터로 분리하는 작업을 수행한다.
    - 이후 확인을 위해 5개의 데이터를 출력해본다.

  ```python
  data_X = data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']].values
  data_y = data['Species'].values
  print(data_X[:5])
  """
  [[5.1 3.5 1.4 0.2]
   [4.9 3.  1.4 0.2]
   [4.7 3.2 1.3 0.2]
   [4.6 3.1 1.5 0.2]
   [5.  3.6 1.4 0.2]
  """
  print(data_y[:5])	# [1 1 1 1 1]
  ```

  - 훈련 데이터와 테스트 데이터를 분리하고 레이블에 대해 원-핫 인코딩을 수행한다.
    - 훈련 데이터와 테스트 데이터를 8:2로 나눈다.

  ```python
  # 훈련 데이터와 테스트 데이터를 나눈다.
  X_train, X_test, y_train, y_test = train_test_split(data_X, data_y, train_size=0.8, random_state=1)
  
  # 원-핫 인코딩을 수행한다.
  y_train = to_categorical(y_train)
  y_test = to_categorical(y_test)
  ```

  - 소프트맥스 회귀 실행하기
    - 입력의 차원이 4이므로 `input_dims`를 4로 설정하고, 출력의 차원이 3이므로 `output_dims`(가장 앞의 인자)를 3으로 설정한다.
    - 활성화 함수는 소프트맥스 함수를 사용하므로 `activation`의 인자값으로 `softmax`를 설정한다.
    - 오차 함수로는 크로스 엔트로피 함수를 사용하는데, 시그모이드 함수를 사용한 이진 분류에서는 `binary_crossentropy`를 사용했지만, 다중 클래스 분류 문제에서는 `categorical_crossentropy`를 사용한다.
    - 옵티마이저로는 경사 하강법의 일종인 adam을 사용한다(특별한 이유가 있는 것은 아니다).
    - 전체 데이터에 대한 훈련 횟수는 200번이다.
    - `validation_data`에 테스트 데이터를 입력하면 각 훈련 횟수마다 테스트 데잍터에 대한 정확도를 출력해준다.
    - 단, 이는 정확도가 측정되고는 있지만 기계는 해당 데이터를 가지고 가중치를 업데이트하지 않는다.
    - 출력에서 accuracy는 훈련 데이터에 대한 정확도이고,  val_accuracy는 테스트 데이터에 대한 정확도를 의미한다.

  ```python
  model = Sequential()
  model.add(Dense(3, input_dim=4, activation='softmax'))
  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
  history = model.fit(X_train, y_train, epochs=200, batch_size=1, validation_data=(X_test, y_test))
  ```

  - 에포크에 따른 정확도 그래프를 출력하려면 아래와 같이 하면 된다.
    - 그래프를 확인해보면 에포크가 증가함에 따라 오차가 점차적으로 감소하는 것을 볼 수 있다.

  ```python
  epochs = range(1, len(history.history['accuracy']) + 1)
  plt.plot(epochs, history.history['loss'])
  plt.plot(epochs, history.history['val_loss'])
  plt.title('model loss')
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(['train', 'val'], loc='upper left')
  plt.show()
  ```

  - 테스트 데이터를 사용하여 정확도를 측정할 수 있다.
    - Keras의 `Sequential.evaluate()` 메서드를 사용하면 된다.

  ```python
  print("\n 테 스 트 정 확 도 : %.4f" % (model.evaluate(X_test, y_test)[1]))
  
  # 테 스 트 정 확 도 : 1.0000
  ```








# 딥 러닝 개요

- 딥 러닝(Deep Learning)
  - 머신 러닝의 특정한 한 분야로서 인공 신경망(aritificial neural network)의 층을 연속적으로 깊게 쌓아올려 데이터를 학습하는 방식이다.
  - 딥 러닝이 화두가 되기 시작한 것은 비교적 최근의 일이지만 딥 러닝의 기본 구조인 인공 신경망의 역사는 생각보다 오래 되었다.



- 퍼셉트론(Perceptron)

  - 프랑크 로젠블라트(Frank Rosenblatt)가 1957년에 제안한 초기 형태의 인공 신경망이다.

    - 다수의 입력으로부터 하나의 결과를 보내는 알고리즘이다.
    - 실제 뇌를 구성하는 신경 세포 뉴런의 동작과 유사한데, 뉴런은 가지 돌기에서 신호를 받아들이고, 이 신호가 일정치 이상의 크기를 가지면 축삭돌기를 통해서 신호를 전달한다.
    - 실제 신경 세포 뉴런에서 신호를 전달하는 축삭돌기의 역할을 퍼셉트론에서는 가중치가 대신한다.
    - 각각의 인공 뉴런에서 보내진 입력값 $x$는 각각의 가중치 $w$와 함께 종착지인 인공 뉴런에 전달된다.
    - 각각의 입력값에는 각각의 가중치기 존재하는데, 가중치의 값이 클수록 해당 입력 값이 중요하다는 것을 의미한다.
    - 각 입력값이 가중치와 곱해져 인공 뉴런에 전달되고, 각 입력값과 그에 해당하는 가중치의 곱의 전체 합이 임계치(threshold)를 넘으면 종착지에 있는 인공 뉴런은 출력 신호로 1을 출력하고, 그렇지 않을 경우 0을 출력한다.
    - 이러한 함수를 계단 함수(step function)라 하며, 계단 함수에 사용된 임계치 값을 수식으로 표현할 때는 보통 세타(Θ)로 표현한다.
    - 이를 식으로 표현하면 아래와 같다.

    $$
    if\sum_i^nw_ix_i \ge Θ \rarr y=1 \\
    if\sum_i^nw_ix_i \lt Θ \rarr y=0
    $$

  - 위 식에서 임계치를 좌변으로 넘기고 편향 $b$(bias)로 표현할 수도 있다.

    - 편향 $b$또한 퍼셉트론의 입력으로 사용되는데, 보통 그림으로 표현할 때는 입력값이 1로 고정되고 편향 $b$가 곱해지는 변수로 표현된다.

    $$
    if\sum_i^nw_ix_i + b \ge 0 \rarr y=1 \\
    if\sum_i^nw_ix_i + b \lt 0 \rarr y=0
    $$

    - 많은 경우에 편의상 편향이 그림이나 수식에서 생략되서 표현되기도 하지만 실제로는 편향 또한 딥 러닝이 최적의 값을 찾아야 할 변수 중 하나이다.

  - 활성화 함수(Activation Function)

    - 뉴런에서 출력값을 변경시키는 함수이다.
    - 즉 뉴런이 입력값과 가중치를 받아 그 값을 어떤 규칙에 따라 변환해 출력하는 함수이다.
    - 초기 인공 신경망 모델인 퍼셉트론은 활성화 함수로 계단 함수를 사용하였지만, 그 뒤에 등장한 여러 발전된 신경망들은 계단 함수 외에도 다양한 활성화 함수를 사용하기 시작했다.
    - 앞에서 살펴본 시그모이드 함수나 소프트맥스 함수 또한 활성화 함수 중 하나이다.

  - 퍼셉트론은 단층 퍼셉트론과 다층 퍼셉트론으로 나뉜다.



- 단층 퍼셉트론(Single-Layer Perceptron)

  - 보내는 단계와 값을 받아서 출력하는 두 단계로만 이루어진 퍼셉트론을 의미한다.
    - 이때 각 단계를 보통 층(layer)라 부르며, 이 두 개의 층을 입력층(input layer)과 출력층(output layer)라고 한다.
  - 컴퓨터는 두 개의 값 0과 1을 입력해 하나의 값을 출력하는 회로가 모여 만들어지는데, 이 회로를 게이트라 부른다.
    - 단층 퍼셉트론을 이용하면 AND, NAND, OR 게이트는 구현가능하다.
    - 게이트 연산에 쓰이는 것은 두 개의 입력값과 하나의 출력값이다.
  - AND 게이트
    - 두 개의 입력값이 각각 0 또는 1의 값을 가질 수 있으면서 모두 1인 경우에만 출력값이 1이 나오는 게이트이다.
    - 단층 퍼셉트론의 식에서 AND 게이트를 만족하는 두 개의 가중치와 편향의 조합은 다양하게 나올 수 있다(아래 예시에서는 임의의 가중치와 편향을 사용했다).
    - 아래 AND 게이트에 가능한 입력값을 모두 넣어보면 오직 두 개의 입력값이 1인 경우에만 1을 출력한다.

  ```python
  def and_gate(x1, x2):
      w1 = 0.5	# x1의 가중치
      w2 = 0.5	# x2의 가중치
      b = -0.7	# 편향
      result = x1*w1 + x2*w2 + b
      if result <= 0:
          return 0
      else:
          return 1
  
  
  print(and_gate(0, 0))	# 0
  print(and_gate(0, 1))	# 0
  print(and_gate(1, 0))	# 0
  print(and_gate(1, 1))	# 1
  ```

  - NAND 게이트
    - 두 개의 입력값이 1인 경우에만 출력값이 0, 나머지 입력값의 쌍에 대해서는 모두 출력값이 1이 나오는 게이트이다.
    - 앞에서 사용한 가중치와 편향값에 -를 붙여서 단층 퍼셉트론의 식에 넣으면 NAND 게이트를 충족한다.

  ```python
  def nand_gate(x1, x2):
      w1 = -0.5	# x1의 가중치
      w2 = -0.5	# x2의 가중치
      b = 0.7	    # 편향
      result = x1*w1 + x2*w2 + b
      if result <= 0:
          return 0
      else:
          return 1
  
  
  print(nand_gate(0, 0))	# 1
  print(nand_gate(0, 1))	# 1
  print(nand_gate(1, 0))	# 1
  print(nand_gate(1, 1))	# 0
  ```

  - OR 게이트
    - 두 개의 입력값이 모두 0인 경우에만 출력값이 0, 나머지 입력값의 쌍에 대해서는 모두 출력값이 1이 나오는 게이트이다.
    - 적절한 가중치와 편향을 찾으면 단층 퍼셉트론의 식으로 구현할 수 있다.

  ```python
  def or_gate(x1, x2):
      w1 = 0.6	    # x1의 가중치
      w2 = 0.6	    # x2의 가중치
      b = -0.5	    # 편향
      result = x1*w1 + x2*w2 + b
      if result <= 0:
          return 0
      else:
          return 1
  
  
  print(or_gate(0, 0))	# 0
  print(or_gate(0, 1))	# 1
  print(or_gate(1, 0))	# 1
  print(or_gate(1, 1))	# 1
  ```

  - 이처럼 가중치와 편향을 변경하는 것 만으로 각기 다른 종류의 게이트를 구현할 수 있다.

    - 위에서 사용한 가중치와 편향 외에도 각 게이트를 충족하게 하는 다양한 가중치와 편향 값들이 있을 것이다.

  - XOR 게이트는 단층 퍼셉트론으로 구현할 수 없다.

    - XOR 게이트는 입력값 두 개가 서로 다른 값을 갖고 있을 때에만 출력값이 1이 되고, 같은 값을 가지면 출력값이 0이 되는 게이트이다.
    - 위의 코드에 아무리 많은 가중치와 편향을 넣어봐도 XOR 게이트를 구현하는 것은 불가능하다.
    - 그 이유는 단층 퍼셉트론은 직선 하나로 두 영역을 나눌 수 있는 문제에 대해서만 구현이 가능하기 때문이다.
    - 예를 들어 AND 게이트에 대한 단층 퍼셉트론을 시각화하면 아래와 같다.
    - 입력값 [0,0], [0,1], [1,0], [1, 1]을 두 부분으로 나누는 직선이 있다.
    - NAND 게이트나 OR 게이트도 마찬가지로 직선으로 나누는 것이 가능하다.

    ![image-20251128175004583](NLP_part3.assets/image-20251128175004583.png)

    - 그러나 XOR 게이트의 경우 직선 하나로 나누는 것은 불가능하고 적어도 두 개의 선이 필요하므로 단층 퍼셉트론으로는 XOR 게이트를 구현할 수 없다.



- 다층 퍼셉트론(MultiLayer Perceptron, MLP)

  - 다층 퍼셉트론은 단층 퍼셉트론에 층이 더 추가된 퍼셉트론이다.
    - 단층 퍼셉트론은 입력층과 출력층만 존재한다.
    - 다층 퍼셉트론은 입력층과 출력층 사이에 층이 추가되며, 이 층을 은닉층(hidden layer)이라고 한다.

  - 다층 퍼셉트론으로 XOR 게이트를 구현하면 아래와 같다.
    - 은닉층 하나에 뉴런(h1, h2)이 두 개인 형태이다.
    - 각각의 뉴런들이 각자의 가중치와 편향을 가진다.


  ```python
def step(x):
    return 1 if x > 0 else 0

def xor_gate(x1, x2):
    # ----- 은닉층 -----
    # h1 = x1 AND (NOT x2)
    z1 = 1.0 * x1 + (-1.0) * x2 + 0.0
    h1 = step(z1)

    # h2 = (NOT x1) AND x2
    z2 = -1.0 * x1 + 1.0 * x2 + 0.0
    h2 = step(z2)

    # ----- 출력층 -----
    z3 = 0.6 * h1 + 0.6 * h2 - 0.5
    return step(z3)

# 테스트
for x1 in [0, 1]:
    for x2 in [0, 1]:
        print(f"x1={x1}, x2={x2} -> {xor_gate(x1, x2)}")
  ```

  - 심층 신경망(Deep Neural Network, DNN)
    - 은닉층이 2개 이상인 신경망을 심층 신경망이라고 한다.
    - 다층 퍼셉트론뿐 아니라 여러 변형된 다양한 신경망들도 은닉층이 2개 이상이면 심층 신경망이라고 한다.
  - 딥 러닝
    - 위에서는 게이트를 만족시키는 같이 가중치와 편향을 직접 찾아 입력했다.
    - 만약 기계가 가중치를 스스로 찾아내도록 자동화한다면 이것이 머신 러닝에서 말하는 훈련 또는 학습 단계에 해당한다.
    - 앞서 살펴보았듯 이 과정에서 손실 함수와 옵티마이저를 사용한다.
    - 만약 학습을 시키는 인공 신경망이 심층 신경망일 경우에는 이를 심층 신경망을 학습시킨다고 하여 딥 러닝이라고 한다.



- 인공 신경망

  - 피드 포워드 신경망(Feed-Forward Neural Network, FFNN)
    - 오직 입력층에서 출력층 방향으로 연산이 전개되는 신경망을 의미한다.
    - 위에서 구현한 XOR 게이트가 이에 해당한다.
  - 순환 신경망(Recurrent Neural Network, RNN)
    - 은닉층의 출력 값을 출력층으로도 보내지만 동시에 은닉층의 출력값이 다시 은닉층의 입력값으로 사용된다.

  - 전결합층(Fully-connected layer, FC, Dense layer)
    - 다층 퍼셉트론은 은닉층과 출력층에 있는 모든 뉴런이 바로 이전 층의 모든 뉴런과 연결되어 있다.
    - 어떤 층의 모든 뉴런이 이전 층의 모든 뉴런과 연결되어 있는 층을 전결합층 또는 완전 연결층이라고 한다.
    - 동일한 의미로 밀집층이라고 부르기도 한다.
  - 활성화 함수(Activation Function)
    - 앞서 살펴본 퍼셉트론에서는 계단 함수를 통해 출력값이 0이 될지, 1이 될지를 결정했다.
    - 이러한 매커니즘은 실제 뇌를 구성하는 신경 세포 뉴런이 전위가 일정치 이상이 되면 시냅스가 화학적으로 연결되는 모습을 모방한 것이다.
    - 이렇게 은닉층과 출력층의 뉴런에서 출력값을 결정하는 함수를 활성화 함수라고 하는데, 계단 함수는 이 활성화 함수 중 하나이다.
  - 인공 신경망에서 활성화 함수는 비선형 함수여야한다.
    - 인공 신경망에서 사용하는 활성화 함수는 선형 함수가 아니라 비선형 함수(nonlinear function)여야 한다.
    - 선형 함수란 출력이 입력의 상수배만큼 변하는 함수를 말한다.
    - 예를 들어 $f(x)=wx+b$라는 함수가 있을 때 $w, b$는 상수로 이 식을 그래프로 만들면 직선이 된다.
    - 반대로 비선형 함수는 직선 1개로는 그릴 수 없는 함수를 말한다.
    - 앞서 퍼셉트론에서도 계단 함수라는 활성화 함수를 사용했는데, 계단 함수 또한 비선형 함수에 속한다.
  - 인공 신경망에서 활성화 함수가 비선형 함수여야 하는 이유
    - 인공 신경망의 능력을 높이기 위해서는 은닉층을 계속해서 추가해야한다.
    - 그런데 만약 활성화 함수로 선형 함수를 사용하게 되면 은닉층을 쌓을 수가 없다.
    - 예를 들어 활성화 함수로 선형 함수를 선택하고 계속 층을 쌓는다고 가정해보자.
    - 이 때 활성화 함수는 $f(x)=wx$라고 가정했을 때, 여기에 은닉층을 두 개 추가한다고 하면 출력층을 포함해서 $y(x)=f(f(f(x)))$가 되고, 이를 식으로 표현하면 $w\times w\times w\times x$이다.
    - 그런데 이는 잘 생각해보면 $w$의 세제곱을  k라고 정의해버리면  $y(x)=kx$와 같이 다시 표현이 가능하다.
    - 이 경우 선형 함수로 은닉층을 여러번 추가하더라도 1회 추가한 것과 차이가 없음을 알 수 있다.
    - 즉 층을 쌓더라도 표현력이 변하지 않는다.



- 주로 사용되는 활성화 함수(비선형 함수)들

  - 계단 함수(Step function)
    - 계단 함수는 실제로는 거의 사용되지 않지만, 배우는 과정에서 많이 사용되는 함수이다.

  ```python
  import numpy as np
  import matplotlib.pyplot as plt
  
  
  def step(x):
      return np.array(x > 0, dtype=int)
  x = np.arange(-5.0, 5.0, 0.1)
  y = step(x)
  plt.title('Step Function')
  plt.plot(x,y)
  plt.show()
  ```

  - 시그모이드 함수(Sigmoid function)
    - 시그모이드 함수의 출력값이 0 또는 1에 가까워지면 그래프의 기울기가 완만해지는 모습을 볼 수 있다.
    - 이 완만해지는 구간에서는 미분값이 0에 가까운 아주 작은 값이고, 가운데 점선 부근의 기울기가 큰 구간의 미분값은 최대값이 0.25이다.
    - 즉 시그모이드 함수를 미분한 값은 0.25 이하의 값이다.
    - 은닉층에서는 거의 사용하지 않고 주로 이진 분류를 위해 출력층에서 사용한다.

  ```python
  import numpy as np
  import matplotlib.pyplot as plt
  
  
  def sigmoid(x):
      return 1/(1+np.exp(-x))
  x = np.arange(-5.0, 5.0, 0.1)
  y = sigmoid(x)
  
  plt.plot(x, y)
  plt.plot([0,0],[1.0,0.0], ':')
  plt.title('Sigmoid Function')
  plt.show()
  ```

  - 시그모이드 함수를 활성 함수로 하는 인공 신경망의 학습 과정
    - 인공 신경망은 입력에 대해서 순전파(forward propagation) 연산을 한다.
    - 순전파 연산을 통해 나온 예측값과 실제값의 오차를 손실 함수를 통해 계산하고, 이 손실(오차,  loss)을 미분하여 기울기를 구한다.
    - 이를 통해 출력층에서 입력층 방향으로 가중치와 편향을 업데이트 하는 과정인 역잔파(back propagation)를 수행한다.
    - 시그모이드 함수의 문제점은 미분을 해서 기울기를 구할 때 발생한다.
  - 기울기 소실(Vanishing gradient) 문제
    - 시그모이드를 활성화 함수로 사용하는 인공 신경망의 층을 쌓는다면, 가중치와 편향을 업데이트 하는 과정에서 0에 가까운 값이 누적해서 곱해지게 되면서 앞단에는 기울기(미분값)가 잘 전달되지 않게 된다.
    - 이러한 현상을 기울기 소실 문제라고 한다.
    - 시그모이드 함수를 사용하는 은닉층의 개수가 많아질경우 0에 가까운 기울기가 계속 곱해지면서 앞단(입력층에 가까운 쪽)에서는 거의 기울기를 전파받을 수 없게 되어(즉  $w$가 업데이트 되지 않아) 학습이 잘 이루어지지 않는다.
    - 따라서 시그모이드 함수를 은닉층에서 사용하는 것은 지양해야 한다.

  - 하이퍼볼릭 탄젠트 함수(Hyperbolic tangent function)
    - 입력값을 -1과 1 사이의 값으로 변환한다.
    - 시그모이드 함수와 같은 기울기 소실 문제가 발생한다.
    - 그러나 시그모이드 함수와는 달리 출력 값이 0을 중심으로 하고 있으며 하이퍼볼릭 탄제느 함수를 미분했을 때의 최대값은 1로 시그모이드 함수의 최대값인 0.25보다는 크다.
    - 따라서 미분했을 때 시그모이드 함수보다는 전반적으로 큰 값이 나오게 되어 기울기 소실 증상이 적은 편이다.

  ```python
  import numpy as np
  import matplotlib.pyplot as plt
  
  
  x = np.arange(-5.0, 5.0, 0.1)
  y = np.tanh(x)
  
  plt.plot(x, y)
  plt.plot([0,0],[1.0,-1.0], ':')
  plt.axhline(y=0, color='orange', linestyle='--')
  plt.title('Tanh Function')
  plt.show()
  ```

  - 렐루 함수(ReLU)
    - 수식은 $f(x)=max(0,x)$로 아주 간단하며, 인공 신경망 은닉층에서 가장 많이 사용되는 함수이다.
    - 음수를 입력하면 0을 출력하고, 양수를 입력하면 입력값을 그대로 반환하는 것이 특징인 함수로, 출력값이 특정 양수값에 수렴하지 않는다.
    - 0 이상의 입력값의 경우에는 미분값이 항상 1이다.
    - 깊은 신경망의 은닉층에서 시그모이드 함수보다 훨씬 더 잘 동작하며, 어떤 연산이 필요한 것이 아니라 단순 임계값이므로 연산 속도도 빠르다.
    - 다만 입력값이 음수면 기울기(미분값)도 0이 된다는 문제가 있으며, 이 뉴런은 다시 회생하는 것이 매우 어렵다.
    - 이 문제를 죽은 렐루(dying ReLU)라고 한다.

  ```python
  import numpy as np
  import matplotlib.pyplot as plt
  
  
  def relu(x):
      return np.maximum(0, x)
  
  x = np.arange(-5.0, 5.0, 0.1)
  y = relu(x)
  
  plt.plot(x, y)
  plt.plot([0,0],[5.0,0.0], ':')
  plt.title('Relu Function')
  plt.show()
  ```

  - 리키 렐루(Leaky ReLU)
    - 죽은 렐루 문제를 보완하기 위해 등장한 함수이다.
    - 입력값이 음수일 경우 0이 아니라 0.001과 같은 매우 작은 수를 반환한다.
    - 수식은  $f(x)=max(ax, x)$로, $a$는 하이퍼파라미터로 새는(leaky) 정도를 의미하며 일반적으로 0.01의 값을 가진다.

  ```py
  import numpy as np
  import matplotlib.pyplot as plt
  
  
  a = 0.01
  
  def leaky_relu(x):
      return np.maximum(a*x, x)
  
  x = np.arange(-5.0, 5.0, 0.1)
  y = leaky_relu(x)
  
  plt.plot(x, y)
  plt.plot([0,0],[5.0,0.0], ':')
  plt.title('Leaky ReLU Function')
  plt.show()
  ```

  - 소프트맥스 함수(Softmax function)
    - 은닉층에서는 ReLU 혹은 그 변형 함수들을 사용하는 것이 일반적이다.
    - 반면 소프트맥스 함수는 시그모이드 함수처럼 출력층에서 주로 사용된다.
    - 시그모이드 함수가 두 가지 선택지 중 하나를 고르는 이진 분류 문제에 사용된다면 소프트맥스 함수는 세 가지 이상의 상호 배타적인 선택지 중 하나를 고르는 다중 클래스 분류 문제에 주로 사용된다.
    - 즉 딥 러닝으로 이진 분류를 할 대는 출력층에서 로지스틱 회귀를 사용하고, 다중 클래스 문제를 풀 때는 소프트맥스 회귀를 사용한다고 생각할 수 있다.

  ```python
  import numpy as np
  import matplotlib.pyplot as plt
  
  
  x = np.arange(-5.0, 5.0, 0.1)
  y = np.exp(x) / np.sum(np.exp(x))
  
  plt.plot(x, y)
  plt.title('Softmax Function')
  plt.show()
  ```



- 행렬곱으로 이해하는 신경망

  - 순전파(Forward Propagation)
    - 인공 신경망에서 입력층에서 출력층 방향으로 예측값의 연산을 진행하는 과정을 순전파라고 한다.
    - 신경망의 순전파는 행렬의 곱셈으로 이해할 수 있다.
  - 입력의 차원이 3, 출력의 차원이 2인 인공 신경망은 아래와 같이 구현이 가능하다.
    - 활성화 함수는 임의로 소프트맥스를 사용하도록 한다.

  ```python
  from tensorflow.keras.models import Sequential
  from tensorflow.keras.layers import Dense
  
  
  model = Sequential()
  model.add(Dense(2, input_dim=3, activation='softmax'))
  ```

  - Keras에서는 `summary()` 메서드를 사용하여 모델에 존재하는 모든 매개변수(가중치와 편향)의 개수를 확인할 수 있다.
    - `summary()` 메서드 내에 출력하는 로직이 포함되어 있다.
    - `Params`를 확인해보면 매개변수의 개수가 8개인 것을 확인할 수 있는데, 학습 가능한 매개변수인 가중치와 편향의 개수가 총 합해서 8개라는 의미이다.

  ```python
  model.summary()
  """
  Model: "sequential"
  ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
  ┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
  ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
  │ dense (Dense)                        │ (None, 2)                   │               8 │
  └──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
   Total params: 8 (32.00 B)
   Trainable params: 8 (32.00 B)
   Non-trainable params: 0 (0.00 B)
  """
  ```

  - 위 내용을 행렬의 곱으로 표현하면 아래와 같다.
    - 입력의 차원이 3, 출력의 차원이 2인데, 이를 신경망의 용어로 표현하면 입력층의 뉴런이 3개, 출력층의 뉴런이 2개이다.
    - 입력층의 3개 뉴런과 출력층의 2개 뉴런 사이에는 총 6개의 연결이 존재하는데, 이는 이 신경망에서 가중치 $w$의 개수가 6개임을 의미한다.
    - 이를 행렬곱 관점에서는 3차원 벡터에서 2차원 벡터가 되기 위해서 3 * 2 행렬을 곱했다고 이해할 수 있다.
    - 그리고 이 행렬 각각의 원소가 각각의 $w$가 되는 것이다.
    - 편향 $b$의 개수는 항상 출력의 차원을 기준으로 확인하면 되는데, 이 인공 신경망의 경우 출력의 차원이 2이므로 편향 또한 두 개이다.
    - 가중치의 개수가 총 6개이고, 편향의 개수가 두 개이므로 총 학습 가능한 매개변수의 수는 8개이다.

  $$
  \begin{bmatrix}
  x_{1}\ \ x_{2} \ \ x_{3}\\
  \end{bmatrix}
  \times
  \begin{bmatrix}
  w_1 \ w_4\\
  w_2 \ w_5\\
  w_3 \ w_6
  \end{bmatrix}
  +
  \begin{bmatrix}
  b_1 \ b_2
  \end{bmatrix}
  =
  \begin{bmatrix}
  y_1 \ y_2
  \end{bmatrix}
  $$

  - 이때, 입력 벡터를 X, 출력 벡터를 W, 가중치 행렬을 W, 편향 벡터를 B라고 하면 인공 신경망은 결국 아래와 같이 표현이 가능하다.

  $$
  Y=XW + B
  $$

  - 행렬곱으로 병렬 연산 이해하기

    - 인공 신경망을 행렬 곱으로 구현하면 병렬 연산도 가능하다.
    - 위의 예시에서는 데이터 중 1개의 샘플만 처리했다고 가정했으나, 인공 신경망이 4개의 샘플을 동시에처리한다고 가정하면, 아래와 같이 표현할 수 있다.

    $$
    \begin{bmatrix}
    x_{1}\ \ x_{2} \ \ x_{3}\\
    x_{1}\ \ x_{2} \ \ x_{3}\\
    x_{1}\ \ x_{2} \ \ x_{3}\\
    x_{1}\ \ x_{2} \ \ x_{3}
    \end{bmatrix}
    \times
    \begin{bmatrix}
    w_1 \ w_4\\
    w_2 \ w_5\\
    w_3 \ w_6
    \end{bmatrix}
    +
    \begin{bmatrix}
    b_1 \ b_2
    \end{bmatrix}
    =
    \begin{bmatrix}
    y_1 \ y_2
    \end{bmatrix}
    $$



- 행렬곱으로 다층 퍼셉트론의 순전파 이해하기

  - 아래와 같은 인경신경망을 Keras로 구현하면 아래와 같다.
    - 입력층은 4개의 입력과 8개의 출력을 가진다.
    - 은닉층1은 8개의 입력과 8개의 출력을 가진다.
    - 은닉층2는 8개의 입력과 3개의 출력을 가진다.
    - 출력층은 3개의 입력과 3개의 출력을 가진다.

  ```python
  from tensorflow.keras.models import Sequential
  from tensorflow.keras.layers import Dense
  
  model = Sequential()
  
  # 4개의 입력과 8개의 출력
  model.add(Dense(8, input_dim=4, activation='relu'))
  
  # 8개의 출력
  model.add(Dense(8, activation='relu'))
  
  # 3개의 출력
  model.add(Dense(3, activation='softmax'))
  ```

  - 배치의 크기가 1일 때 입력층과 은닉층1 사이에 생기는 가중치와 편향 행렬의 크기는 아래와 같이 구할 수 있다.

    - 입력 행렬 X의 크기는 1 * 4(배치 크기 * 벡터의 차원)이고, 출력은 8개이므로, 출력 행렬 Y의 크기는 1 * 8이 된다($X_{1\times 4} \times W_{n \times j} + B_{m \times j}=Y_{1 \times 8}$)

    - 그런데 가중치 행렬 W의 행은 입력 행렬 X의 열과 같아야 한다($X_{1\times 4} \times W_{4 \times j} + B_{m \times j}=Y_{1 \times 8}$).
    - 편향 행렬 B는 출력행렬 Y의 크기에 영향을 주지 않으므로 편향 행렬 B의 크기는 출력 행렬 Y의 크기와 같다($X_{1\times 4} \times W_{4 \times j} + B_{1 \times 8}=Y_{1 \times 8}$).
    - 가중치 행렬 W의 열은 출력 행렬 Y의 열과 동일해야한다($X_{1\times 4} \times W_{4 \times 8} + B_{1 \times 8}=Y_{1 \times 8}$).

  - 은닉층 1과 은닉층2 사이에 생기는 가중치완 편향 행렬의 크기, 은닉층2와 출력층 사이에 생기근 가중치와 편향의 크기도 위와 같은 방법으로 구하면 아래와 같다.

    - 은닉층1과 은닉층2: $X_{1\times 8} \times W_{8 \times 3} + B_{1 \times 8}=Y_{1 \times 8}$
    - 은닉층2와 출력층: $X_{1\times 8} \times W_{8 \times 3} + B_{1 \times 3}=Y_{1 \times 3}$







## 딥러닝의 학습 방법

- 손실 함수(Loss function)
  - 실제 값과 예측값의 차이를 수치화해주는 함수이다.
    - 두 값의 차이(오차)가 클 수록 손실 함수의 값은 크고, 오차가 작을 수록 손실 함수의 값은 작아진다.
    - 회귀에서는 평균 제곱 오차, 분류 문제에서는 크로스 엔트로피를 손실 함수로 주로 사용한다.
    - 손실 함수의 값을 최소화하는 두 개의 매개변수인 가중치와 편향을 찾는 것이 딥 러닝의 학습 과정이므로 손실 함수의 선정은 매우 중요하다.
  - MSE(Mean Squared Error, MSE)
    - 평균 제곱 오차는 앞서 선형 회귀에서 봤던 손실 함수이다.
    - 연속형 변수를 예측할 때 사용된다.
    - 딥 러닝을 사용한 자연어 처리는 대부분 분류 문제이므로 MSE보다는 크로스 엔트로피 함수들을 주로 사용한다.
  - 이진 크로스 엔트로피(Binary Cross-Entropy)
    - 이항 교차 엔트로피라고도 부르는 손실 함수이다.
    - 출력층에서 시그모이드 함수를 사용하는 이진 분류의 경우 손실 함수로 이진 크로스 엔트로피를 사용한다.
  - 크로스 엔트로피(Categorical Cross-Entropy)
    - 범주형 교차 엔트로피라고도 부르는 손실 함수이다.
    - 출력층에서 소프트맥스 함수를 사용하는 다중 클래스 분류일 경우 손실 함수로 크로스 엔트로피를 사용한다.
  - 이 밖에도 다양한 손실 함수들이 있다.
    - 위에서 살펴본 것들은 자주 사용하는 대표적인 손실 함수들이다.
    - [텐서 플로우 공식 문서](https://www.tensorflow.org/api_docs/python/tf/keras/losses)에서 다양한 손실 함수들을 확인할 수 있다.



- 배치 크기에 따른 경사 하강법
  - 배치(Batch)
    - 가중치 등의 매개 변수의 값을 조정하기 위해 사용하는 데이터의 양을 말한다.
    - 전체 데이터를 가지고 매개 변수의 값을 조정할 수도 있고, 정해진 양의 데이터만 가지고 매개 변수의 값을 조정할 수 있다.
  - 손실 함수의 값을 줄여나가면서 학습하는 방법은 어떤 옵티마이저를 사용하느냐에 따라 달라진다.
  - 배치 경사 하강법(Batch Gradient Descent)
    - 가장 기본적인 경사 하강법이다.
    - 옵티마이저 중 하나로 오차를 구할 때 전체 데이터를 고려한다.
    - 딥 러닝에서는 전체 데이터에 대한 한 번의 훈련 횟수를 1 에포크라고 하는데, 배치 경사 하강법은 한 번의 에포크에 모든 매개 변수 업데이트를 단 한 번 수행한다.
    - 배치 경사 하강법은 전체 데이터를 고려해서 학습하므로 한 번의 매개 변수 업데이트에 시간이 오래 걸리며 메모리를 크게 요구한다는 단점이 있다.
  - 배치 크기가 1인 확률적 경사 하강법(Stochastic Gradient Descent, SGD)
    - 배치 경사 하강법은 전체 데이터에 대해 계산을 하여 시간이 너무 오래 걸린다는 문제를 해결하기 위해 등장한 방법이다.
    - 매개 변수 값을 조정시 전체 데이터가 아니라 무작위로 선택한 하나의 데이터에 대해서만 계산하는 방법이다.
    - 더 적은 데이터를 사용하므로 더 빠르게 계산이 가능하다.
    - 다만 매개 변수의 변경폭이 불안정하고 때로는 경사 하강법보다 정확도가 낮을 수 있다는 단점이 있다.
  - 미니 배치 경사 하강법(Mini-Batch Gradient Descent)
    - 배치 크기를 지정하여 해당 데이터 개수만큼에 대해서 계산하여 매개 변수의 값을 조정하는 방법이다.
    - 배치 경사 하강법보다 빠르며 SGD보다 안정적이라는 장점이 있다.
    - 가장 많이 사용되는 경사 하강법이다.
    - 배치의 크기는 2의 n제곱에 해당하는 숫자로 선택하는 것이 일반적이다.



- 옵티마이저(Optimizer)

  - 모멘텀(Momentum)
    - 관성이라는 물리학의 법칙을 응용한 방법이다.
    - 모멘텀은 경사 하강법에서 계산된 접선의 기울기에 한 시점 전의 접선의 기울기 값을 일정한 비율만큼 반영한다.
    - 이렇게하면 마치 언덕에서 공이 내려올 때 중간에 작은 웅덩이에 빠지더라도 관성의 힘으로 넘어서는 효과를 줄 수 있다.
    - 모멘텀을 통해 로컬 미니멈에 도달하더라도 관성의 힘으로 값이 조절되면서 로컬 미니멈에서 탈출하여 글로벌 미니멈 혹은 더 낮은 로컬 미니멈으로 갈 수 있게 된다.
  - 아다그라드(Adagrad)
  
    - 매개 변수들은 각자 의미하는 바가 다른데, 모든 매개 변수에 동일한 학습률을 적용하는 것은 비효율적이다.
    - 아다그라드는 각 매개 변수에 각기 다른 학습률을 적용시킨다.
    - 이때 변화가 많은 매개 변수는 학습률이 작게 설정되고 변화가 적은 매개 변수는 학습률을 높게 설정시킨다.
  - RMSprop
        - 아다그라드는 학습을 계속 진행할 경우 학습률이 지나치게 떨어진다는 단점이 있다.
        - RMSprop는 이러한 단점을 다른 수식으로 대체하여 개선한다.
  - Adam
        - RMSprop와 momentum을 합친 듯한 방법이다.
        - 방향과 학습률 두 가지를 모두 잡기 위한 방법이다.
  - 모두 tensorflow에서 사용이 가능하다.
  
  ```python
  import tensorflow as tf
  
  tf.keras.optimizers.SGD(lr=0.01, momentum=0.9)
  tf.keras.optimizers.Adagrad(lr=0.01, epsilon=1e-6)
  tf.keras.optimizers.Adagrad(lr=0.01, rho=0.9, epsilon=1e-06)
  tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
  ```



- 에포크와 배치 크기와 이터레이션

  - 학습

    - 기계가 실제값과 예측값의 오차로부터 옵티마이저를 통해서 가중치를 업데이트하는 과정이다.
    - 현실의 학습을 비유하자면 문제지의 문제를 풀고 정답지의 정답을 보고 채점을 하면서 부족했던 부분을 업데이트하는 과정이다.

  - 에포크(Epoch)

    - 인공 신경망에서 전체 데이터에 대해서 순전파와 역전파가 끝난 상태를 말한다.
    - 전체 데이터를 하나의 문제지에 비유한다면 문제지의 모든 문제를 끝까지 다 풀고, 정담지로 채점을 하여 문제지에 대한 공부를 한 번 끝낸 상태를 말한다.
    - 만약 에포크가 10이라면 전체 데이터 단위로는 총 10번 학습한다.
    - 문제지를 10번 푼 셈이다.
    - 이 에포크 횟수가 지나치게 많거나 적으면 과적합이나 과소적합이 발생할 수 있다.

  - 배치 크기(Batch size)

    - 몇 개의 데이터 단위로 매개 변수를 업데이트 하는지를 말한다.
    - 현실에 비유하면 문제지에서 몇 개씩 문제를 풀고 나서 정답지를 확인하는지를 의미한다.
    - 예를 들어 100문제가 수록되어 있는 문제지의 문제를 10개 단위로 풀고 채점한다면 배치 크기는 10이다.
    - 기계는 배치 크기가 10이면 10개의 샘플 단위로 가중치를 업데이트 한다.

  - 이터레이션(Iteration) 또는 스텝(Step)

    - 한 번의 에포크를 끝내기 위해서 필요한 배치의 수, 또는 한 번의 에포크 내에서 이루어지는 매개 변수의 업데이트 횟수를 의미한다.

    - 배치의 크기와 배치의 수는 다른 개념이다.
    - 전체 데이터가 100일때, 배치 크기가 5라면 배치의 수는 20이며, 이는 한 번의 에포크 당 매개 변수 업데이트가 20번 실행된다는 것을 의미한다.
    - 스텝이라고 부르기도 한다.



### 역전파

- 예시를 위해 사용할 인경 신경망은 아래와 같다.

  - 입력층, 은닉층, 출력층 3개의 층을 가지며 각기 두 개의 입력과 두 개의 은닉층, 출력층 뉴런을 사용한다.

  ![img](NLP_part3.assets/backpropagation_1.png)

  - 은닉층과 출력층의 모든 뉴런은 활성화 함수로 시그모이드 함수를 사용한다.
  - 은닉층과 출력층의 모든 뉴런에 변수 $z$가 있는데, $z$는 이전층의 모든 입력이 각각의 가중치와 곱해진 값들이 모두 더해진 가중합을 의미한다.
  - 이 값은 뉴런에서 아직 시그모이드 함수를 거치지 않은 상태이다(즉 활성화 함수의 입력을 의미한다).
  - 변수 $h$ 또는 $o$는 $z$가 시그모이드 함수를 지난 후의 값으로 각 뉴런의 출력값을 의미한다.
  - 이번 역전파 예시에서는 인공 신경망에 존재하는 모든 가중치 $w$에 대해서 역전파를 통해 업데이트하는 것을 목표로 하며, 편향은 고려하지 않는다.



- 순전파(Forward Propagation)

  - 입력층에서 출력층으로 향하면서 가중치를 업데이트하는 과정이다.
    - 주어진 값이 아래 그림과 같을 때 순전파를 진행하는 과정을 살펴본다.
    - 파란색 숫자는 입력값을 의미하며, 빨간색 숫자는 각 가중치의 값을 의미한다.

  ![img](NLP_part3.assets/backpropagation_2.png)

  - 각 입력은 입력층에서 은닉층 방향으로 향하면서 각 입력에 해당하는 가중치와 곱해지고, 결과적으로 가중합으로 계산되어 은닉층 뉴런의 시그모이드 함수의 입력값이 된다.
    - $z_1, z_2$는 시그모이드 함수의 입력으로 사용되는 각각의 값에 해당한다.

  $$
  z_1 = w_1x_1+w_2x_2=0.3\times0.1 + 0.25\times0.2=0.08 \\
  z_2 = w_3x_1+w_4x_2=0.4\times0.1 + 0.35\times0.2=0.11
  $$

  - $z_1, z_2$는 각각의 은닉층 뉴런에서 시그모이드 함수를 지나게 된다.
    - 시그모이드 함수가 반환하는 결과값은 은닉층 뉴런의 최종 출력값이다.
    - 식에서는 $h_1, h_2$에 해당하며 아래의 결과와 같다.

  $$
  h_1=sigmoid(z_1)=0.51998934 \\
  h_2=sigmoid(z_2)=0.52747230
  $$

  - $h_1, h_2$는 다시 출력층의 뉴런으로 향하게 된다.
    - 이 때 다시 각각의 값에 해당되는 가중치와 곱해지고, 다시 가중합이 되어 출력층 뉴런의 시그모이드 함수의 입력값이 된다.

  $$
  z_3 = w_5h_1+w_6h_2=0.45\times h_1 + 0.25\times h_2=0.44498412 \\
  z_4 = w_7h_1+w_8h_2=0.7\times h_1 + 0.35\times h_2=0.68047592
  $$

  - $z_3, z_4$는 출력층 뉴런에서 시그모이드 함수를 지나 인공 신경망이 최종적으로 계산한 출력값이 된다.
    - 이 값을 실제값을 예측하기 위한 값으로서 예측값이라고 부른다.

  $$
  o_1=sigmoid(z_3)=0.60944600 \\
  o_2=sigmoid(z_4)=0.66384491
  $$

  - 이제 예측값과 실제값의 오차를 계산하기 위한 오차 함수를 선택해야한다.
    - 오차를 계산하기 위한 손실 함수로는 MSE를 사용한다.
    - 아래 식에서는 실제 값을 target으로 표현했고, 순전파를 통해 나온 예측값을 output으로 표현했다.
    - 각 오차를 모두 더하면 전체 오차 $E_{total}$이 된다.

  $$
  E_{o1}={1\over2}(target_{o1}-output_{o1})^2=0.02193381 \\
  E_{o2}={1\over2}(target_{o2}-output_{o2})^2=0.00203809 \\
  E_{total}=E_{o1}+E_{o2} = 0.02397190
  $$



- 역전파 1단계

  - 출력층에서 입력층 방향으로 계산하면서 가중치를 업데이트하는 과정이다.

    - 출력층 바로 이전의 은닉층을  N층이라고 가정한다.
    - 출력층과  N층 사이의 가중치를 업데이트하는 단계를 역전파 1단계라 하자.
    - N층과 N층의 이전층 사이의 가중치를 업데이트 하는 과정을 역전파 2단계라고 하자.

  - 역전파 1단계에서 업데이트 해야 할 가중치는 총 4개이다.

    - 즉 출력층에서 은닉층으로 향하는 가중치 4개($w_5, w_6, w_7, w_8$)를 업데이트해야한다.
    - 원리 자체는 동일하므로 우선 $w_5$에 대해 먼저 업데이트를 진행해본다.
    - 경사 하강법을 수행하려면 가중치 $w_5$를 업데이트하기 위해서 $\partial E_{total}\over \partial w_5$를 계산해야한다.
    - 이를 계산하기 위해 미분의 연쇄 법칙에 따라 아래와 같이 풀어쓸 수 있다.

    $$
    {\partial E_{total}\over \partial w_5}={\partial E_{total}\over \partial o_1} \times {\partial o_1 \over \partial z_3} \times {\partial z_3 \over \partial w_5}
    $$

    - 여기서 $E_{total}$의 값은 앞서 순전파를 진행하고 계산했던 전체 오차값으로 식은 아래와 같다.

    $$
    E_{total} = E_{o1}={1\over2}(target_{o1}-output_{o1})^2+
    E_{o2}={1\over2}(target_{o2}-output_{o2})^2
    $$

    - 따라서 첫 번째 항인 $\partial E_{total}\over \partial o_1$은 아래와 같다.

    $$
    {\partial E_{total}\over \partial o_1} = 2 \times {1 \over 2}(target_{o1}-output_{o1})^{2-1}\times(-1)+0 \\
    {\partial E_{total}\over \partial o_1} = -(target_{o1}-output_{o1}) = -(0.4-0.60944600) = 0.20944600
    $$

    - 두 번째 항을 보면 $o_1$이라는 값은 시그모이드 함수의 출력값인데, 시그모이드 함수의 미분은 $f(x)\times(1-f(x))$이다.

    $$
    {\partial o_1 \over \partial z_3} = o_1 \times(1-o_1)=0.60944600(1-0.60944600) = 0.23802157
    $$

    - 마지막으로 세 번째 항은 $h_1$의 값과 동일하다.

    $$
    {\partial z_3 \over \partial w_5} = h_1 = 0.51998934
    $$

    - 이제 우변의 값을 모두 곱하면 된다.

    $$
    {\partial E_{total}\over \partial w_5} = 0.20944600 \times 0.23802157 \times 0.51998934 = 0.02592286
    $$

    - 최종적으로 경하 하강법을 통해 가중치를 업데이트 하는 과정은 아래와 같다(학습률은 0,5라고 가정한다).

    $$
    w_5^+ = w_5 - \alpha{\partial E_{total}\over \partial w_5} = 0.45-0.5\times 0.02592286 = 0.43703857
    $$

  - 위 과정을 통해 모든 가중치를 업데이트 한 결과는 아래와 같다.
    $$
    {\partial E_{total}\over \partial w_6}={\partial E_{total}\over \partial o_1} \times {\partial o_1 \over \partial z_3} \times {\partial z_3 \over \partial w_6} \rarr w_6^+ = 0.38685205\\
    {\partial E_{total}\over \partial w_7}={\partial E_{total}\over \partial o_2} \times {\partial o_2 \over \partial z_4} \times {\partial z_4 \over \partial w_7} \rarr w_7^+ = 0.69629578\\
    {\partial E_{total}\over \partial w_8}={\partial E_{total}\over \partial o_2} \times {\partial o_2 \over \partial z_4} \times {\partial z_4 \over \partial w_8} \rarr w_8^+ = 0.59624247
    $$



- 역전파 2단계

  - 예시에서는 은닉층이 1개밖에 없으므로 은닉층에서 입력층으로의 이동이 유일한 역전파 2단계이다.

    - 만약 은닉층이 더 많은 경우라면 입력층 방향으로 한 단계씩 계속해서 계산해가야한다.

  - 이번 단계에서 계산할 가중치는 $w_1,w_2,w_3,w_4$이다.

    - 원리 자체는 동일하므로 $w_1$에 대해 먼저 업데이트를 진행한다.
    - 경사하강법을 수행하려면 $w_1$를 업데이트하기 위해 $\partial E_{total}\over \partial w_1$을 계산해야한다.
    - 미분의 연쇄 법칙에 따라 아래와 같이 풀어 쓸 수 있다.

    $$
    {\partial E_{total}\over \partial w_1}={\partial E_{total}\over \partial h_1} \times {\partial h_1 \over \partial z_1} \times {\partial z_1 \over \partial w_1}
    $$

    - 위 식에서 우변의 첫 번째 항은 다시 아래와 같이 풀어쓸 수 있다.

    $$
    {\partial E_{total}\over \partial h_1} = {\partial E_{o_1} \over \partial h_1} \times {\partial E_{o_2} \over \partial h_1}
    $$

    - 첫 번째 항 ${\partial E_{o_1} \over \partial h_1}$에 대해 분해 및 계산하면 아래와 같다.

    $$
    {\partial E_{o_1} \over \partial h_1} = {\partial E_{o_1} \over \partial z_3} \times {\partial z_3 \over \partial h_1} = {\partial E_{o_1} \over \partial o_1} \times {\partial o_1 \over \partial z_3} \times {\partial z_3 \over \partial h_1} \\
    = 0.20944600 \times 0.23802157 \times 0.45 = 0.02243370
    $$

    - 같은 방식으로 ${\partial E_{o_2} \over \partial h_1}$도 계산하면 아래와 같다.

    $$
    {\partial E_{o_2} \over \partial h_1} = {\partial E_{o_2} \over \partial z_4} \times {\partial z_4 \over \partial h_1} = {\partial E_{o_2} \over \partial o_2} \times {\partial o_2 \over \partial z_4} \times {\partial z_4 \over \partial h_1} \\
     = 0.00997311
    $$

    

    - 결국 ${\partial E_{total}\over \partial h_1}$의 값은 아래와 같다.

    $$
    {\partial E_{total}\over \partial h_1} = 0.02243370 + 0.00997311 = 0.03240681
    $$

    - 나머지 두 항에 대해서 구하면 아래와 같다.

    $$
    {\partial h_1 \over \partial z_1}=h_1 \times(1-h_1) = 0.51998934(1-0.51998934)=0.24960043 \\ {\partial z_1 \over \partial w_1} = x_1 = 0.1
    $$

    - 결국 ${\partial E_{total}\over \partial w_1}$는 아래와 같다.

    $$
    {\partial E_{total}\over \partial w_1} = 0.03240681×0.24960043×0.1=0.00080888
    $$

    - 최종적으로 경사 하강법을 통해 가중치를 업데이트할 수 있다.

    $$
    w_1^+ = w_1-\alpha{\partial E_{total} \over \partial w_1}=0.3-0.5\times0.00080888=0.29959556
    $$

  - 위 과정을 통해 모든 가중치를 업데이트 한 결과는 아래와 같다.

  $$
  {\partial E_{total}\over \partial w_2}={\partial E_{total}\over \partial h_1} \times {\partial h_1 \over \partial z_1} \times {\partial z_1 \over \partial w_2} \rarr w_2^+ = 0.24919112 \\
  {\partial E_{total}\over \partial w_3}={\partial E_{total}\over \partial h_2} \times {\partial h_2 \over \partial z_2} \times {\partial z_2 \over \partial w_3} \rarr w_3^+ = 0.39964496 \\
  {\partial E_{total}\over \partial w_4}={\partial E_{total}\over \partial h_2} \times {\partial h_2 \over \partial z_2} \times {\partial z_2 \over \partial w_4} \rarr w_4^+ = 0.34928991
  $$



- 결과 확인

  - 업데이트 된 가중치에 대해 다시 한 번 순전파를 진행하여 오차가 감소했는지 확인한다.
    - 기존의 오차였던 0.02397190보다 오차가 감소하한 것을 확인할 수 있다.

  $$
  z_1=w_1x_1+w_2x_2=0.29959556 \times 0.1+0.24919112 \times 0.2=0.07979778 \\
  z_2=w_3x_1 + w_4x_2=0.39964496 \times 0.1 + 0.34928991 \times 0.2= 0.10982248 \\
  h_1 = sigmoid(z_1) = 0.51993887 \\
  h_2 = sigmoid(z_2) = 0.52742806 \\
  z_3=w_5h_1+w_6h_2=0.43703857 \times h_1+0.38685205 \times h_2=0.43126996 \\
  z_4=w_7h_1+w_8h_2=0.69629578 \times h_1+0.59624247 \times h_2=0.67650625 \\
  E_{o1} = {1 \over 2}(target_{o1}-output_{o1})^2=0.02125445 \\
  E_{o2} = {1 \over 2}(target_{o2}-output_{o2})^2=0.00198189 \\
  E_{total}=E_{o1}+E_{o2} = 0.02323634
  $$

  - 인공신경망의 학습은 이처럼 오차를 최소화하는 가중치를 찾기 위해 순전파와 역전파를 반복하는 것을 말한다.



- 인공 신경망의 과적합을 막는 방법

  - 과적합이 문제가 되는 이유

    - 모델이 과적합되면 훈련 데이터에 대한 정확도는 높을지라도 새로운 데이터에 대해서는 제대로 동작하지 않는다.
    - 이는 모델이 학습 데이터를 과하게 암기하여 훈련 데이터에 포함된 노이즈까지 학습한 상태라고 해석할 수 있ㄷ다.

  - 데이터의 양을 늘리기

    - 모델은 데이터의 양이 적을 경우, 해당 데이터의 특정 패턴이나 노이즈까지 쉽게 암기하게 되므로 과적합 현상이 발생할 확률이 증가한다.
    - 따라서 데이터의 양을 늘릴 수록 모델은 데이터의 일반적인 패턴을 학습하게 되어 과적합을 방지할 수 있다.
    - 만약 데이터의 양이 적을 경우에는 의도적으로 기존의 데이터를 조금씩 변형하고 추가하여 데이터의 양을 늘리기도 하는데, 이를 데이터 증식 또는 증강(data augmentation)이라고 한다.
    - 텍스트 데이터의 경우에는 데이터를 증강하기 위해 번역 후 재변역을 통해 새로운 데이터를 만들어내는 역번역(back translation) 등의 방법을 사용하기도 한다.

  - 모델의 복잡도 줄이기

    - 인공 신경망의 복잡도는 은닉층의 수나 매개 변수의 수 등으로 결정된다.
    - 과적합 현상이 포착되었을 때, 인공 신경망 모델에 대해서 할 수 있는 한 가지 조치는 인공 신경망의 복잡도를 줄이는 것이다.
    - 인공 신경망에서는 모델에 있는 매개 변수들의 수르 모델의 수용력(capacity)라고 하기도 한다.

  - 가중치 규제(regularization) 적용하기

    - 복잡한 모델이 간단한 모델보다 과적합될 가능성이 높다.
    - 그리고 간단한 모델은 적은 수의 매개 변수를 가진 모델을 말한다.
    - 복잡한 모델을 좀 더 간단하게 하는 방법으로 가중치 규제가 있다.

    - L1 규제는 가중치 w들의 절대값 합계를 비용 함수에 추가하는 방식으로  L1 노름이라고도 한다.
    - 즉, 기존의 비용 함수에 모든 가중치에 대해서  $\lambda|w|$를 더 한 값을 비용 함수로 한다.
    - L2 규제는 모든 가중치 w들의 제곱합을 비용 합수에 추가하는 방식으로 L2 노름이라고도 한다.
    - 즉, 기존의 비용 함수에 모든 가중치에 대해 ${1\over 2}\lambda w^2
      $를 더한 값을 비용 함수로 한다.
    - $\lambda$는 규제의 강도를 정하는 하이퍼 파라미터로, 크다는 것은 모델이 훈련 데이터에 대해서 적합한 매개 변수를 찾는 것보다 규제를 위해 추가된 항들을 작게 유지하는 것을 우선한다는 의미가 된다.
    - 위 두 식 모두 비용 함수를 최소화하기 위해서는 가중치 w들의 값이 작아져야 한다는 특징이 있다.
    - L1 규제를 예로 들면 비용 함수가 최소가 되게 하는 가중치와 편향을 찾는 동시에 가중치들의 절대값의 합도 최소가 되어야 한다.
    - 이렇게 되면 가중치 w의 값들은 0 또는 0에 가까이 작아져야 하므로 어떤 특성들은 모델을 만들 때 거의 사용되지 않게 된다.
    - 예를 들어 $H(X)=w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4$라는 수식이 있을 때, 여기에 L1규제를 적용하면 $w_3$의 값이 0이 된다고 해보자.
    - 이는 $x_3$의 특성은 사실 모델의 결과에 별 영향을 주지 못하는 특성임을 의미한다.
    - L2 규제는 L1 규제와는 달리 가중치들의 제곱을 최소화하므로 w의 값이 완전히 0이 되기보다는 0에 가까워지는 경향을 띈다.
    - L1 규제는 어떤 특성들이 모델에 영향을 주고 있는지를 정확히 판단하고자 할 때 유용하다.
    - 만약 이런 판단을 위한 것이 아니라면 경험적으로는 L2 규제가 더 잘 동작하므로 L2 규제를 더 권장한다.
    - 인공 신경망에서 L2 규제는 가중치 감쇠(weight_decay)라고도 부른다.

  - 드롭아웃

    - 학습 과정에서 신경망의 일부를 사용하지 않는 방법이다.
    - 예를 들어 드롭아웃의 비율을 0.5로 한다면 학습 과정마다 랜덤으로 절반의 뉴런을 사용하지 않고, 절반의 뉴런만을 사용한다.
    - 드롭아웃은 신경망 학습 시에만 사용하고, 예측 시에는 사용하지 않는 것이 일반적이다.
    - 학습 시에 인공 신경망이 특정 뉴런 또는 특정 조합에 너무 의존적이게 되는 것을 방지해주고, 매번 랜덤 선택으로 특정 뉴런들을 사용하지 않으므로 서로 다른 신경망들을 앙상블하여 사용하는 것 같은 효과를 내어 과적합을 방지한다.
    - 케라스에서는 아래와 같이 드롭아웃을 추가할 수 있다.

  ```python
  from tensorflow.keras.models import Sequential
  from tensorflow.keras.layers import Dropout, Dense
  
  max_words = 10000
  num_classes = 46
  
  model = Sequential()
  model.add(Dense(256, input_shape=(max_words,), activation='relu'))
  model.add(Dropout(0.5)) # 드롭아웃 추가
  model.add(Dense(128, activation='relu'))
  model.add(Dropout(0.5)) # 드롭아웃 추가
  model.add(Dense(num_classes, activation='softmax'))
  ```



### 기울기 소실과 폭주

- 기울기 소실(Gradient Vanishing)과 기울기 폭주(Gradient Exploding)
  - 기울기 소실
    - 깊은 인공 신경망을 학습하다보면 역전파 과정에서 입력층으로 갈 수록 기울기가 점차적으로 작아지는 현상이 발생할 수 있다.
    - 입력층에 가까운 층들에서 가중치들이 업데이트가 제대로 되지 않으면 결국 최적의 모델을 찾을 수 없게 되는데, 이를 기울기 소실이라 한다.
  - 기울기 폭주
    - 기울기가 점차 커지더니 가중치들이 비정상적으로 큰 값이 되면서 결국 발산하게 되는 현상도 발생할 수 있다.
    - 이를 기울기 폭주라고 하며 순환 신경망에서 쉽게 발생할 수 있다.



- ReLU와 그 변형들
  - 활성화 함수로 시그모이드 함수를 사용하는 경우 기울기 소실이 발생할 수 있다.
    - 시그모이드 함수를 사용하면 입력의 절대값이 클 경우에 시그모이드 함수의 출력값이 0 또는 1에 수렴하면서 기울기가 0에 가까워진다.
    - 따라서 역전파 과정에서 전파 시킬 기울기가 점차 사라져서 입력층 방향으로 갈 수록 제대로 역전파가 되지 않는 기울기 소실 문제가 발생할 수 있다.
  - 기울기 소실을 완화하는 방법
    - 은닉층의 활성화 함수로 시그모이드나 하이퍼볼릭탄젠트 함수 대신에 ReLU나 ReLU의 변형 함수와 같은 Leaky ReLU를 사용한다.
    - Leaky ReLU를 사용하면 모든 입력값에 대해서 기울기가 0에 수렴하지 않아 죽은 ReLU 문제를 해결한다.
    - 은닉층에서는 ReLU나 Leaky ReLU와 같은 ReLU 함수의 변형들을 사용한다.



- 그래디언트 클리핑(Gradient Clipping)

  - 그래디언트 클리핑은 말 그대로 기울기 값을 자르는 것을 의미한다.
    - 기울기 폭주를 막기 위해 임계값을 넘지 않도록 값을 자른다.
    - 즉 임계치만큼 크기를 감소시킨다. 
  - 이는 뒤에서 배울 신경망인 RNN에서 유용하게 사용한다. 
    - RNN은 역전파 과정에서 시점을 역행하면서 기울기를 구하는데, 이때 기울기가 너무 커질 수 있기 때문이다. 
    - 케라스에서는 아래과 같은 방법으로 그래디언트 클리핑을 수행한다.

  ```python
  from tensorflow.keras import optimizers
  
  Adam = optimizers.Adam(lr=0.0001, clipnorm=1.)
  ```



- 가중치 초기화(Weight initialization)

  - 가중치 초기화가 중요한 이유

    - 같은 모델을 훈련시키더라도 가중치가 초기에 어떤 값을 가졌느냐에 따라서 모델의 훈련 결과가 달라지기도 한다. 
    - 즉 가중치 초기화만 적절히 해줘도 기울기 소실 문제과 같은 문제를 완화시킬 수 있다.

  - 세이비어 초기화(Xavier initialization)

    - 2010년 세이비어 글로럿과 요슈아 벤지오는 가중치 초기화가 모델에 미치는 영향을 분석하여 제안한 초기화 방법이다. 
    - 이 초기화 방법은 제안한 사람의 이름을 따서 세이비어(Xavier Initialization) 초기화 또는 글로럿 초기화(Glorot Initialization)라고 한다.
    - 이 방법은 균등 분포(Uniform Distribution) 또는 정규 분포(Normal distribution)로 초기화 할 때 두 가지 경우로 나뉘며, 이전 층의 뉴런 개수와 다음 층의 뉴런 개수를 가지고 식을 세운다.
    - 이전 층의 뉴런의 개수를 $n_{in}$, 다음층의 뉴런의 개수를 $n_{out}$이라고 할 때 균등 분포를 사용하여 가중치를 초기화할 경우 아래와 같은 균등 분포 범위를 사용하라고 한다.

    $$
    W \sim Uniform(-\sqrt{6 \over n_{in}+n_{out}} + \sqrt{6 \over n_{in}+n_{out}})
    $$

    - 즉 $\sqrt{6 \over n_{in} + n_{out}}$을 m이라고 했을 때 -m과 +m 사이의 균등 분포를 의미한다.
    - 정규 분포로 초기화할 경우에는 평균이 0이고, 표준  편차 $\sigma$가 아래를 만족하도록 한다.

    $$
    \sigma = \sqrt{2\over n_{in}+n_{out}}
    $$

    - 세이비어 초기화는 여러 층의 기울기 분산 사이에 균형을 맞춰서 특정 층이 너무 주목을 받거나 다른 층이 뒤쳐지는 것을 막는다. 
    - 그런데 세이비어 초기화는 시그모이드 함수나 하이퍼볼릭 탄젠트 함수와 같은 S자 형태인 활성화 함수와 함께 사용할 경우에는 좋은 성능을 보이지만, ReLU와 함께 사용할 경우에는 성능이 좋지 않다.
    - 따라서 이럴 경우 He 초기화 같은 다른 방법을 사용한다.

  - He 초기화(He initialization)

    - He 초기화(He initialization)는 세이비어 초기화와 유사하게 정규 분포와 균등 분포 두 가지 경우로 나뉜다. 
    - 단, He 초기화는 세이비어 초기화와 다르게 다음 층의 뉴런의 수를 반영하지 않는다. 
    - 전과 같이 이전 층의 뉴런의 개수를 $n_{in}$이라고 할 때, He 초기화는 균등 분포로 초기화 할 경우에 다음과 같은 균등 분포 범위를 가지도록 한다.

    $$
    W \sim Uniform(-\sqrt{6 \over n_{in}} + \sqrt{6 \over n_{in}})
    $$

    - 정규분포로 초기화할 경우 표준 편차 $\sigma$가 아래를 만족하도록 한다.

    $$
    \sigma = \sqrt{2\over n_{in}}
    $$

    - 시그모이드 함수나 하이퍼볼릭탄젠트 함수를 사용할 경우에는 세이비어 초기화 방법이 효율적이지만 ReLU 계열 함수를 사용할 경우에는 He 초기화 방법이 효율적이다.
    - ReLU 계열 함수와 He 초기화 방법을 조합하여 사용하는 것이 좀 더 보편적이다.



- 배치 정규화(Batch Normalization)

  - 내부 공변량 변화(Internal Covariate Shift)
    - 배치 정규화를 이해하기 위해 이해해야 하는 개념이다.
    - 공변량 변화는 훈련 데이터의 분포와 테스트 데이터의 분포가 다른 경우를 의미한다.
    - 내부 공변량 변화는 신경망 층 사이에서 발생하는 입력 데이터의 분포 변화를 의미한다.
    - 즉, 학습 과정에서 층 별로 입력 데이터 분포가 달라지는 현상를 말한다.
    - 이전 층들의 학습에 의해 이전 층의 가중치 값이 바뀌게 되면, 현재 층에 전달되는 입력 데이터의 분포가 현재 층이 학습했던 시즘의 분포와 차이가 발생한다.
    - 배치 정규화를 제안한 논문에서는 기울기 소실과 폭주와 같은 딥 러닝 모델의 불안정성이 층마다 입력의 분포가 달라지기 때문이라고 주장한다.
  - 배치 정규화
    - 한 번에 들어오는 배치 단위로 정규화하는 것을 말한다.
    - 인공 신경망의 각 층에 들어가는 입력을 평균과 분산으로 정규화하여 학습을 효율적으로 만드는 방식이다.
    - 배치 정규화는 각 층에서 활성화 함수를 통과하기 전에 수행된다. 
    - 배치 정규화를 요약하면 입력에 대해 평균을 0으로 만들고, 정규화를 한 뒤, 정규화 된 데이터에 대해서 스케일과 시프트를 수행하는 것이다. 
    - 이때 두 개의 매개변수 γ와 β를 사용하는데, γ는 스케일을 위해 사용하고, β는 시프트를 하는 것에 사용하며 다음 레이어에 일정한 범위의 값들만 잔달되도록 한다.
    - 학습 시 배치 단위의 평균과 분산들을 차례로 받아 이동 평균과 이동 분산을 저장해 두었다가 테스트 할 때는 해당 배치의 평균과 분산을 구하지 않고 저장해둔 평균과 분산으로 정규화를 한다.
    - 미니 배치란 동일한 특성(feature) 개수를 가진 다수의 샘플들을 의미한다.

  - 배치 정규화를 시각화하면 아래와 같다.

  ![img](NLP_part3.assets/batch.png)

  - 배치 졍규화의 수식
    - BN은 배치 정규화를 의미한다.
    - Input은 미니 배치 $B={x^{(1)}, x^{(1)},...,x^{(m)}}$
    - Output은 $y^{(i)}=BN_{\gamma,\beta}(x^{(i)})$
    - 미니 배치에 대한 평균 계산: $\mu_B \larr{1 \over m}\sum_{i=1}^m x^{(i)}$
    - 미니 배치에 대한 분산 계산: $\sigma_B^2 \larr {1 \over m} \sum_{i=1}^m(x^{(i)}-\mu_B)^2$
    - 정규화: $\hat x^{(i)} \larr {x^{(i)}-\mu_B \over \sqrt{\sigma_B^2+\epsilon}}$
    - m은 미니 배치에 있는 샘플의 수.
    - $\mu_B$는 미니 배치 B에 대한 평균.
    - $\sigma_B$는 미니 배치 B에 대한 표준 편차.
    - $\hat x^{(i)}$는 평균이 0이고 정규화된 입력 데이터.
    - $\epsilon$은 $\sigma^2$가 0일 때, 분모가 0이 되는 것을 막는 작은 양수로, 보편적으로 $10^{-5}$를 사용한다.
    - $\gamma$는 정규화 된 데이터에 대한 스케일 매개변수로 학습 대상이다.
    - $\beta$는 정규화 된 데이터에 대한 시프트 매개변수로 학습 대상이다.
    - $y^{(i)}$는 스케일과 시프트를 통해 조정한 BN의 최종 결과이다.

  - 배치 정규화의 장점
    - 시그모이드 함수나 하이퍼볼릭탄젠트 함수를 사용하더라도 기울기 소실 문제를 개선할 수 있다.
    - 가중치 초기화에 훨씬 덜 민감해진다.
    - 훨씬 큰 학습률을 사용할 수 있어 학습 속도를 개선하는 것이 가능하다.
    - 미니 배치마다 평균과 표준편차를 계산하여 사용하므로 훈련ㅇ 데이터에 일종의 잡음 주입의 부수 효과로 과적합을 방지하는 효과도 낸다.
    - 즉 마치 드롭아웃과 비슷한 효과를 낸다.
  - 배치 정규화 사용시 고려 사항
    - 모델을 복잡하게 하며 추가 계산을 하는 것이므로 테스트 데이터에 대한 예측 시에 실행 시간이 느려진다.
    - 따라서 서비스 속도를 고려하는 관점에서는 배치 정규화가 꼭 필요한지 검토해야한다.
  - 배치 정규화의 한계
    - 미니 배치 크기에 의존적인데, 너무 작은 배치 크기에서는 잘 동작하지 않을 수 있다.
    - 단적으로 배치 크기를 1로 하게되면 분산은 0이 된다.
    - 작은 미니 배치에서는 배치 정규화의 효과가 극단적으로 작용되어 훈련에 악영향을 줄 수 있다.
    - 배치 정규화를 적용할때는 작은 미니 배치보다는 크기가 어느정도 되는 미니 배치에서 하는 것이 좋다.
    - 또한 RNN에 적용하기 어렵다는 단점이 있다.
    - RNN은 각 시점(time step)마다 다른 통계치를 가지는데 이는 RNN에 배치 정규화를 적용하는 것을 어렵게 만든다. 



- 층 정규화

  - 배치 크기에도 의존적이지 않으며, RNN에도 적용하는 것이 수월한 정규화 방식이다.
  - 층 정규화를 시각화하면 아래와 같다.

![img](NLP_part3.assets/layer.png)
















# 소프트맥스 회귀

- 소프트맥스 회귀(Sotfmax Regression)
  - 앞에서 살펴본 로지스틱 회귀는 2개의 선택지 중에서 1개를 고르는 이진 분류를 푸는 방식이었다.
    - 로지스틱 회귀에서 사용한 시그모이드 함수는 입력된 데이터에 대해서 0과 1 사이의 값을 출력하여 해당 값이 둘 중 하나에 속할 확률로 해석할 수 있도록 만들어줬다.
    - 예를 들어 0이 정상 메일, 1이 스팸 메일이라고 정의해놓는다면 시그모이드 함수의 0과 1 사이의 출력값을 스팸 메일일 확률로 해석할 수 있었다.
    - 확률값이 0.5를 넘으면 1에 가까우니 스팸 메일로 판단하면 되고, 그 반대라면 정상 메일로 판단하면 된다.
  - 소프트맥스 회귀는 3개 이상의 선택지 중에서 1개를 고르는 다중 클래스 분류 문제를 푸는 방식이다.
    - 이진 분류가 두 개의 선택지 중 하나를 고르는 문제였다면, 세 개 이상의 선택지 중 하나를 고르는 문제를 다중 클래스 분류라고 한다.
    - 전체 선택지에 걸친 확률의 합계가 1이 되도록 하여 전체 선택지 중 가장 확률이 높은 선택지를 선택하도록 하는 소프트맥스 함수를 사용한다.



- 소프트맥스 함수(Softmax function)

  - 선택해야 하는 선택지의 총 개수를 k라고 할 때, k차원의 벡터를 입력 받아 각 클래스에 대한 확률을 추정한다.
    - k차원의 벡터에서 i번째 원소를 $z_i$, i번째 클래스가 정답일 확률을 $p_i$로 나타낼 때, 소프트맥스 함수는 $p_i$를 아래와 같이 정의한다.

  $$
  p_i={e^{z_i}\over \sum_{j=1}^k}for\ \ i=1,2,...k
  $$

  - 예를 들어 꽃에 대한 데이터가 있을 때 그 꽃이 a, b, c 중 어떤 꽃일지 예측하는 경우를 생각해보자.
    - 이 경우 k는 3이므로 3차원 벡터 $z=[z_1\ z_2\ z_3]$의 입력을 받으면 소프트맥스 함수는 아래와 같은 출력을 반환한다.
    - $p_1,p_2,p_3$ 각각은 1~3번 클래스(setosa, versicolor, virginica)가 정답일 확률을 나타내며, 각각 0과 1사이의 값으로 총합은 1이 된다.

  $$
  softmax(z)=[{e^{z_1} \over \sum_{j=1}^3e^{z_j}}{e^{z_2} \over \sum_{j=1}^3e^{z_j}}{e^{z_3} \over \sum_{j=1}^3e^{z_j}}]=[p_1,p_2,p_3]=\hat y = 예측값
  $$

  - 이 때, 꽃에 대한 데이터는 꽃받침 길이와 넓이, 꽃의 길이, 꽃잎 넓이가 있다고 하자.
    - 이 4가지 요소가 독립변수 $x_1,x_2,x_3,x_4$가 된다.
    - 이는 모델이 4차원 벡터를 입력으로 받음을 의미한다.
    - 그런데 소프트맥스 함수의 입력으로 사용되는 벡터는 차원이 분류하고자 하는 클래스의 개수(k, 예시의 경우 3)가 되어야 하므로 어떤 가중치 연산을 통해 3차원 벡터로 변환되어야한다.
  - 샘플 데이터를 소프트맥스 함수의 입력 벡터로 차원을 축소하는 방법
    - 샘플 데이터(독립 변수 벡터)를 소프트맥스 함수의 입력 벡터로 차원을 축소하는 방법은 간단하다.
    - 소프트맥스 함수의 입력 벡터 $z$의 차원수만큼 결과값이 나오도록 가중치 곱을 진행한다.
    - 경우의 수는 4*3으로 총 12개이며 전부 다른 가중치를 가지고, 학습 과정에서 점차적으로 오차를 최소화하는 가중치로 값이 변경된다.
  - 오차를 계산하는 방법
    - 소프트맥스 함수의 출력은 분류하고자하는 클래스의 개수만큼 차원을 가지는 벡터로 각 원소는 0과 1사이의 값을 가지며, 각각은 특정 클래스가 정답일 확률을 나타낸다.
    - 그렇다면 이 예측값과 비교를 할 수 있는 실제값의 표현 방법이 있어야하는데, 소프트맥스 회귀에서는 실제값을 원-핫 벡터로 표현한다.
    - a,b,c의 원-핫 벡터를 각기 [1,0,0], [0,1,0], [0,0,1]이라고 하자.
    - 이 때, 현재 풀고 있는 샘플 데이터의 실제 값이 b라면 b의 원-핫 벡터는 [0,1,0]이다.
    - 이 경우 예측값과 실제값의 오차가 0이 되는 경우는 소프트맥스 함수의 결과가 [0, 1, 0]이 되는 경우이다.
    - 이 두 벡터의 오차를 계산하기 위해서 소프트맥스 회귀는 비용 함수로 크로스 엔트로피 함수를 사용한다.
  - 선형 회귀나 로지스틱 회귀와 마찬가지로 소프트맥스 회귀 역시 오차로부터 가중치를 업데이트한다.
    - 더 정확히는 선형회귀나 로지스틱 회귀와 마찬가지로 편향 또한 업데이트 대상이 되는 매개 변수이다.



- 원-핫 벡터를 사용하는 이유

  - 꼭 실제값을 원-핫 벡터로 표현해야만 다중 클래스 분류 문제를 풀 수 있는 것은 아니다.

    - 다만 대부분의 다중 클래스 분류 문제가 각 클래스 간의 관계가 균등하다는 점에서 원-핫 벡터는 이러한 점을 표현할 수 있는 적절한 표현 방법이다.

  - 다수의 클래스를 분류하는 문제에서는 이진 분류처럼 2개의 숫자 레이블이 아니라 클래스의 개수만큼 숫자 레이블이 필요하다.

    - 이때 직관적으로 생각해볼 수 있는 레이블링 방법은 분류해야 할 클래스 전체에 정수 인코딩을 하는 것이다.
    - 예를 들어 분류해야 할 레이블이 red, green, blue와 같이 3개라면 각각 0,1,2로 레이블한다.
    - 또한 분류해야 할 클래스가 north, east, west, south 네 개이고, 인덱스를 숫자 1부터 시작하고 싶다면 1,2,3,4로 레이블 해볼 수 이싸.

  - 그런데 일반적인 다중 클래스 분류 문제에서 레이블링 방법으로는 위와 같은 정수 인코딩이 아니라 원-핫 인코딩을 사용하는 것이 보다 클래스의 성질을 잘 표현했다고 할 수 있다.

    - 예를 들어 a, b, c라는 3개의 클래스가 있는 문제가 있다고 가정해보자.
    - 레이블은 정수 인코딩을 사용하여 각각 1,2,3을 부여하였다.
    - 손실 함수로 평균 제곱 오차를 사용하면 정수 인코딩이 오해를 불러일으킬 수 있다.
    - 아래는 앞서 살펴본 평균 제곱 오차의 식을 가져온 것이다.

    $$
    {1 \over n}\sum_{i}^n(y_i-\hat y_i)^2
    $$

    - 직관적인 오차 크기 비교를 위해 평균을 구하는 수식은 제외하고 제곱 오차로만 판단하면 실제 값이 b일때 예측값이 a였다면 제곱 오차는 $(2-1)^2=1$이 된다.
    - 실제값이 c일때 예측값이 a였다면 제곱 오차는 $(3-1)^2=4$가 된다.
    - 즉 b와 c 사이의 오차보다 c와 a 사이의 오차가 더 크다.
    - 이는 기계에게 b와 c가 a와 c보다 더 가깝다는 정보를 주는 것과 같다.

  - 원-핫 벡터의 무작위성

    - 일반적인 분류 문제에서 각 클래스는 순서의 의미를 갖고 있지 않으므로 각 클래스간 오차는 균등한 것이 맞다.
    - 정수 인코딩과 달리 원-핫 인코딩은 분류 문제의 모든 클래스 간의 관계를 균등하게 분배한다.
    - 아래는 세 개의 카테고리에 대해서 원-핫 인코딩을 통해서 레이블을 했을 때 각 클래스 간의 제곱 오차가 균등합을 보여준다.
    - $((1,0,0)-(0,1,0))^2=2$, $((1,0,0)-(0,0,1))^2=2$
    - 다르게 표현하면 모든 클래스에 대해서 원-핫 인코딩을 통해 얻은 원-핫 벡터들은 모든 쌍에 대해 유클리드 거리를 구해도 전부 유클리드 거리가 동일하다.
    - 원-핫 벡터는 이처럼 각 클래스의 표현 방법이 무작위성을 가진다는 점을 표현할 수 있다.



- 비용 함수(Cost function)

  - 크로스 엔트로피 함수
    - 소프트맥스 회귀에서는 비용 함수로 크로스 엔트로피 함수를 사용한다.
    - 아래 식에서 $y$는 실제값, $k$는 클래스의 개수, $y_j$는 실제 원-핫 벡터의 $j$번째 인덱스, $p_j$는 샘플 데이터가 $j$번째 클래스일 확률을 나타낸다.
    - $p_j$는 표기에 따라서 $\hat y_j$로 표현하기도 한다.

  $$
  cost = -\sum_{j=1}^ky_jlog(p_j)
  $$

  - 크로스 엔트로피 함수를 비용 함수로 사용하는 이유
    - $c$가 실제값 원-핫 벡터에서 1을 가진 원소의 인덱스라고 한다면, $p_c=1$은 $\hat y$가 $y$를 정확하게 예측한 경우가 된다.
    - 이를 식에 대입해보면 $-1log(1)=0$이 되기 때문에, 결과적으로  $\hat y$가 $y$를 정확하게 예측한 경우의 크로스 엔트로피 함수의 값은 0이 된다.
    - 즉 , 크로스 엔트로피 함수의 출력값을 최소회 히는 방향으로 학습해야한다.
  - 이를 n개의 전체 데이터에 대한 평균을 구한다고하면 최종 비용 함수는 아래와 같다.

  $$
  cost = -{1 \over n}\sum_{i=1}^n \sum_{j=1}^k y_j^{(i)}log(p_j^{(i)})
  $$

  - 이진 분류에서의 크로스 엔트로피 함수

    - 로지스틱 회귀에서 봤던 크로스 엔트로피 함수식과 달라보이지만 본질적으로 동일한 함수식이다.
    - 로지스틱 회귀의 크로스 엔트로피 함수식으로부터 소프트맥스 회귀의 크로스 엔트로피 함수식을 도출하면 아래와 같다.

    $$
    cost = -(ylogH(X)+(1-y)log(1-H(X)))
    $$

    - 위 식은 앞서 로지스틱 회귀에서 봤던 크로스 엔트로피의 함수식을 보여주는데, 위 식에서 $y$를 $y_1$, $1-y$를 $y_2$로 치환하고 $H(X)$를 $p_1$, $1-H(X)$를 $p_2$로 치환하면 아래의 수식을 얻을 수 있다.

    $$
    -(y_1log(p_1)+y_2\ log(p_2))
    $$

    - 위 식은 아래와 같이 표현할 수 있다.

    $$
    -(\sum_{i=1}^2y_i\ log\ p_i)
    $$

    - 소프트맥스 회귀에서는 k의 값이 고정된 값이 아니므로 2를 k로 변경해야한다.

    $$
    -(\sum_{i=1}^ky_i\ log\ p_i)
    $$

    

    - 위 식은 결과적으로 소프트맥스 회귀의 식과 동일하며, 역으로 소프트맥스 회귀에서 로지스틱 회귀의 크로스 엔트로피 함수식을 얻는 것은 k를 2로하고 를 $y_1$, $y_2$를 $y$와 $y-1$로 치환하고, $p_1$, $p_2$를 $H(X)$, $1-H(X)$로 치환하면 된다.
    - 정리하면 소프트맥스 함수의 최종 비용 함수에서 k가 이라고 가정하면 결국 로지스틱 회귀의 비용 함수와 같다.

    $$
    cost = -{1 \over n}\sum_{i=1}^n \sum_{j=1}^k y_j^{(i)}log(p_j^{(i)})=-{1 \over n} \sum_{i=1}^n[y^{(i)}log(p^{(i)})+(i-y^{(i)})log(1-p^{(i)})]
    $$


























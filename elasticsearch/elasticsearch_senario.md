# ES 클러스터 구축 시나리오

- ES는 주로 두 가지 용도로 사용한다.
  - 분석 엔진으로 ES 클러스터를 구성
    - 하루에 적재되는 문서의 전체 용량과 보간 기간을 반드시 고려해야 한다.
  - 검색 엔진으로 ES 클러스터를 구성
    - 여러 개의 인덱스를 사용하지 않는 것이 일반적이다.
    - 검색 서비스로 제공할 인덱스 몇 개만 지속적으로 사용하는 게 일반적
    - 분석 엔진과 달리 응답 시간이 가장 중요한 요소가 된다.



## 시나리오1. 일 100GB 데이터 분석용 클러스터

- 시나리오
  - 하루에 100GB 정도의 데이터를 저장하면서 보관 기간이 한 달인 분석 엔진 클러스터
  - 인덱스 이름 패턴: elasticsearch-YYYY.MM.dd
  - 프라이머리 샤드 기준 하루에 색인되는 인덱스의 용량: 100GB * 1(일) =100GB
  - 인덱스 보관 기간: 30일
  - 레플리카 샤드 개수: 1개
  - 클러스터에 저장되는 전체 예상 용량 100GB*30(일) = 6TB
  - 데이터 노드 한 대에 저장할 수 있는 용량: 2TB
  - 클러스터에 저장될 인덱스의 총 개수: 30개
    - 일별 한 개씩 인덱스를 생성하고, 보관 기간이 30일이므로 총 30개의 인덱스가 생성된다.



- 데이터 노드의 개수 정하기
  - 전체 예상 용량이 6TB이고 노드 한 대에 저장할 수 있는 용량이 2TB라고 가정했으므로 데이터 노드 3대가 있으면 될 것 같지만 그렇지 않다.
    - ES는 기본적으로 노드의 디스크 사용률을 기준으로 샤드를 배치한다.
    - 노드의 디스크 사용률이 low watermark 기본값인 85%가 넘으면 해당 노드에 샤드 할당을 지양한다.
    - 따라서 노드의 최대 데이터 적재 용량을 80%로 잡는다.
  - 6TB의 용량이 전체 용량의 80%가 되어야 하기 때문에 클러스터의 전체 용량은 7.5TB로 산정할 수 있다.
    - 따라서 데이터 노드의 개수도 3개가 아닌 4개가 되어야 한다.
    - 하지만 노드의 디스크 사용량 측면에서는 이렇게 정상적인 상황만 고려하면 안된다.
    - 데이터 노드 한 대가 장애가 발생했다고 가정했을 때, 레플리카를 1로 설정했으므로 한 대의 노드 장애에 대해서는 yellow 상태롤 보장한다.
    - 하지만 노드의 장애가 장기화 될 경우 해당 노드에 저장된 문서들을 다른 노드에서 충분히 받아줄 수 있을 만큼의 용량을 확보해야 한다.
  - 노드의 장애 상황을 고려
    - 데이터 노드가 4대일 때에는 총 8TB가 확보되어 6TB의 데이터를 수용하는데 문제가 없지만 노드 한 대에 장애가 발생해서 클러스터에 노드가 3대만 남게 되면 모든 데이터 노드의 디스크가 가득 차게 된다.
    - 하지만 5대로 구성하면 총 10TB가 확보되어 데이터 노드 한 대에 장애가 발생하더라도 8TB까지 저장할 수 있기 때문에 데이터 노드 장애를 시급하게 복구하지 않아도 된다.
    - 즉 데이터 노드의 장애 상황까지 고려하여 클러스터를 총 5대의 데이터 노드로 구성해야 한다.



- 샤드의 개수 정하기

  - 보통 분석 엔진에서는 샤드 하나의 크기를 20~40G 정도로 할당하는 것을 권고한다.
    - 이보다 더 커지는 것은 문제가 되지만, 작은 용량으로 산정하는 것은 성능상 문제가 되지 않는다.
  - 샤드의 개수는 샤드 할당을 고려하여 노드의 배수로 설정하는 것을 권장한다.

  - 하루에 100GB 정도의 데이터가 인덱스에 색인 되므로 5개의 샤드면 적절하지만 넉넉잡아서 10개로 설정할 경우
    - 하나의 인덱스를 구성하는 프라이머리 샤드의 개수: 10개
    - 클러스터의 전체 인덱스에 의해 생성되는 샤드의 총 개수: `10(개)*30(일)*2(레플리카) = 600개`
    - 인덱스 하나를 기준으로 노드 한 대에 할당되는 샤드 개수: `10(개)*2(레플리카)/5(노드 개수)=4개`
    - 인덱스 전체를 기준으로 노드 한 대에 할당되는 샤드의 총 개수: `10(개)*2(레플리카)*30(일)/5(노드의 개수)`



## 시나리오2. 일 1GB의 데이터 분석과 장기간 보관용 클러스터

- 시나리오
  - 하루 1GB 미만의 소량의 데이터를 저장하면서 보관 기간이 3년인 클러스터를 가정
  - 인덱스 이름 패턴: elasticsearch-YYYY.MM(월별로 보관)
    - 일별로 보관할 경우 3년이면 1095개의 인덱스가 생성되고 이에 따라 전체 샤드 수가 증가하게 되어 마스터 노드에 부담이 된다.
  - 프라이머리 샤드 기준 한 달에 색인되는 인덱스의 용량: 1GB * 30(일)=30GB
  - 인덱스 보관 기간: 3년
  - 레플리카 샤드 개수: 1개
  - 클러스터에 저장되는 전체 예상 용량: 30GB*36(개월)\*2(레플리카) = 2.16TB
  - 데이터 노드 한 대에 할당할 수 있는 용량: SSD 2TB
  - 클러스터에 저장될 인덱스의 총 개수:36개



- 데이터 노드의 수
  - 전체 용량을 고려했을 때 2대면 충분하지만 노드 한 대에서 장애가 발생했을 경우 노드가 한 대만 존재하게 되기 때문에 3대로 산정한다.
  - 노드가 3대이기에 가용한 용량이 꽤 많이 남는데 이는 레플리카샤드를 추가하여 안정성을 강화하는 형태로 활용할 수 있다.



- 분석 엔진에 유용한 성능 확보 방법들
  - 색인이 끝난 인덱스는 forcemerge API로 검색 성능 확보
  - 색인이 끝난 인덱스는 read only로 설정하여 Shard Request Cache가 삭제되지 않도록 설정
  - 조회한지 1년이 지난 인덱스는 read only 설정을 해제하고 close API로 클러스터의 open 샤드에서 제외 처리



## 시나리오3. 일 100GB의 데이터 분석과 장기간 보관 클러스터

- 시나리오
  - 하루에 100GB 이상의 큰 데이터를 저장하면서 보관 기간이 1년인 클러스터
    - 시나리오 1과 2의 어려운 부분이 결합된 시나리오
  - 인덱스 이름 패턴: elasticsearch-YYYY.MM.dd
  - 프라이머리 샤드 기준 하루에 색인되는 인덱스의 용량: 100GB * 1(일) =100GB
  - 인덱스 보관 기간: 1년
  - 레플리카 샤드 개수: 1개
  - 클러스터에 저장되는 전체 예상 용량: 100GB*365(일)\*2(레플리카) = 73TB
  - 데이터 노드 한 대에 할당할 수 있는 용량: SSD 2TB
  - 클러스터에 저장될 인덱스의 총 개수:365개



- hotdata/warmdata

  - hotdata/warmdata 영역에 데이터를 분산 저장
    - 보통 이렇게 오랜 기간 저장하는 데이터는 모든 데이터를 자주 분석하지는 않는다.
    - 따라서 자주 조회하게 될 최근 데이터는 hotdata 영역에 저장하고. 자주 보지는 않지만 연간 분석을 위해 가끔씩 조회하게 될 데이터는 warmdata 영역에 저장하는 방식을 취하면 사용자의 데이터 보관과 비용 절감이라는 두 마리 토끼를 잡을 수 있다.
  - hotdata/warmdata는 ES가 기본적으로 지원하는 기능은 아니다.
    - 샤드 배치를 통해 hotdata/warmdata 처럼 사용하는 것이다.

  - 보통 빠른 응답을 위해 SSD 디스크 사용을 권장하지만 warmdata 구성에 사용할 디스크는 상대적으로 저렴하면서 고용량의 저장 공간을 제공하는 SATA 디스크를 사용한다.
  - hotdata/warmdata 노드의 데이터 보관 기간 설정
    - hotdata 노드 인덱스 보관 기간: 1개월
    - hotdata 노드에 저장되는 전체 예상 용량: 100GB*30(일)\*2(레플리카)=6TB
    - warmdata  노드 한 대에 할당할 수 있는 용량: SATA 10TB
    - warmdata 노드 인덱스 보관 기간: 11개월
    - warmdata 노드에 저장되는 전체 예상 용량: 100GB*335(일)\*2(레플리카)=67TB



- 설정 방법

  - elasticsearch.yml 파일에서 노드별로 설정

  ```yaml
  # hotdata 설정
  node.attr.box_type: hotdata
  
  # warmdata 설정
  node.attr.box_type: warmdata
  ```

  - 새로 생성되는 인덱스는 hotdata 노드에만 배치되도록 설정
    - box_type이 hotdata로 설정된 노드에만 샤드가 할당되도록 설정한다.

  ```bash
  $ curl -XPUT "localhost:9200/인덱스명?pretty" -H 'Content-type: application/json' -d '
  {
  	"settings":{
  		"index":{
  			"routing.allocation.require.box_type":"hotdata"
  		}
  	}
  }
  '
  ```

  - 아예 새로 생성되는 인덱스는 모두 hotdata 노드에만 배치되도록 템플릿을 생성하는 것도 방법이다.

  ```bash
  $ curl -XPUT "localhost:9200/_template/템플릿명?pretty" -H 'Content-type: application/json' -d '
  {
  	"index_pattenrs":[*], # 새로 생성되는 모든 인덱스 대상
  	"order":1,
  	"settings":{
  		"routing.allocation.require.box_type":"hotdata"
  	},
  }
  '
  ```

  - 이후 한 달이 지난 인덱스는 warndata 노드로 샤드를 재배치한다.

  ```bash
  $ curl -XPUT "localhost:9200/인덱스명/_settings?pretty" -H 'Content-type: application/json' -d '
  {
  	"settings":{
  		"index":{
  			"routing.allocation.require.box_type":"warmdata "
  		}
  	}
  }
  '
  ```



- 위 조건을 바탕으로 클러스터 볼륨 현황과 노드 대수를 산정
  - hotdata 노드 대수: 5대
  - hotdata 노드 한 대의 볼륨 할당률:6TB/10TB*100=60%
  - hotdata 노드 한 대 장애 시 데이터 노드 한 대의 볼륨 할당률: 6TB/8TB*100=75%
  - warmdata 노드 대수:10대
  - warmdata 노드 한 대의 볼륨 할당률: 67TB/100TB*100=67%
  - warmdata 노드 한 대 장애 시 데이터 노드 한 대의 볼륨 할당률:67TB/90TB*100=74%



- 샤드 설정

  - 인덱스를 구성하는 프라이머리 샤드의 개수: 10개
    - 조회가 빈번하게 일어나는 hotdata 노드 수량의 2배인 10개로 산정
  - 클러스터 전체 인덱스에 의해 생성되는 샤드의 총 개수: 10(개)*365(일)\*2(레플리카)=7300개
  - 인덱스 전체를 기준으로 hotdata 노드 한 대에 할당되는 샤드의 총 개수: 10개*2(레플리카 샤드)\*30(일)/5(노드 개수):120개
  - 인덱스 전체를 기준으로 warmdata 노드 한 대에 할당되는 샤드의 총 개수: 10개*2(레플리카 샤드)\*335(일)/10(노드 개수):670개

  - hotdata 노드와 warmdata 노드의 개수가 다르기 때문에 hotdata 노드에서 warmdata 노드로 넘어갈 때 샤드가 불균등하게 배치될 수 있다.
    - 이를 막기 위해서는 hotdata 노드의 개수와 warmdata 노드의 개수의 최소공배수로 설정하여 인덱스를 생성하면 된다.



- curator를 통해 hotdata 노드에서 warmdata 노드로의 이동을 자동화 할 수 있다.



- warmdata 노드를 생성할 수 없는 상황이라면 스냅샷 기능을 활용해 유사한 효과를 내는 것이 가능하다.



## 시나리오4. 검색 엔진으로 활용하는 클러스터

- 시나리오
  - 100ms 내에 검색 결과가 제공되어야 한다.
    - 데이터를 가지고 있는 노드가 모두 100ms 내에 응답해야 한다.
    - 즉 데이터 노드 한 대가 반드시 100ms 내에 응답을 주어야 하며, 모든 노드가 동일한 성능을 보장해야 한다는 이야기이다.
  - 검색 엔진에 사용할 데이터는 500GB이다.
  - 검색 요구사항이 변경되어 매핑이 변경될 수 있다.



- 검색 엔진의 특성
  - 분석 엔진과 달리 빠른 응답 속도가 서비스의 질을 좌우한다.
  - 많은 인덱스를 사용하는 경우가 드물다.
  - 검색 엔진에 사용할 데이터는 이미 준비된 상태에서 서비스한다.
  - 인덱스의 필드가 추가되거나 필드 데이터 타입이 변경되는 등의 매핑 변경이 있을 때 인덱스를 새롭게 생성 후 재색인하여 운영한다.



- 검색 요청이 처리되는 과정
  - ES는 기본적으로 1 쿼리, 1샤드, 1 스레드를 기준으로 검색한다.
  - 3개의 노드에 샤드가 각각 하나씩 있고, 각 노드에는 4개의 CPU 코어가 있다고 가정
    - 검색 요청을 처리하기 위한 검색 스레드 풀에는 CPU 코어 수와 같은 수인 4개의 스레드가 생성되어 있다.
    - 사용자가 4개의 검색 요청을 날리면 요청을 가장 처음 받은 노드가 검색 스레드풀에서 스레드를 하나 꺼내서 자신이 가지고 있는 샤드에 요청에 해당하는 문서가 있는지 찾아본다.
    - 그와 동시에 다른 두 노드에도 검색 요청에 해당하는 문서가 있는지 찾아봐달라는 요청을 보낸다.
  - 3개의 노드에 샤드가 4개씩 있고, 각 노드에는 4개의 CPU 코어가 있다고 가정
    - 이 경우 검색 요청이 하나만 들어와도 각 노드에 있는 4 개의 스레드가 각각 4개의 샤드에서 문서를 찾기 때문에 모든 스레드를 사용하게 된다.
    - 따라서 3개의 요청은 처리되지 못하고 검색 스레드 큐에 위치한다.
    - 만약 큐가 가득차면 rejected 현상을 일으킬 수도 있다.
    - 따라서 샤드가 지나치게 많으면 검색 성능을 저하시키는 원인이 된다.
    - 따라서 클러스터를 검색 엔진으로 구축할 경우 성능 테스트를 통해 적정한 수준의 샤드 수를 결정해야 한다.



- 노드의 개수
  - 검색 엔진의 경우는 클러스터의 전체 디스크 사용량이 크게 중요하지 않다.
    - 디스크 사용량보다는 스레드풀로 사용할 CPU 코어와 메모리를 충분히 확보해 두는 편이 중요하다.
    - 시나리오상의 검색 엔진은 전체 데이터가 요청된다 해도 500GB의 용량만 사용할 뿐이다.
  - 노드 한대당 30GB의 힙 메모리를 할당해 놓았다고 가정
    - 만일 적절한 샤드 수가 40개라면 인덱스 전체 크기인 500GB 보다 크면서 샤드수의 약수인 값인 20을 데이터 노드의 개수로 설정한다.
    - 따라서 클러스터 전체의 힙 사이즈는 600GB가 되고, 모든 데이터를 검색하는 쿼리가 입력되었을 때 Full GC가 발생하지 않게 된다(30*20=600>500).
    - 이 경우 노드 하나당 2개의 샤드가 할당되게 된다.



- 매핑이 변경될 경우
  - 사용 중인 인덱스의 매핑을 변경하기 어렵기에 새롭게 인덱스를 생성하고 데이터를 재색인해야 한다.
  - 그러나 인덱스의 이름은 고유해야 하기 때문에 기존 검색 요청을 처리하던 인덱스 이름의 변경이 필요하다.
  - 이럴 경우 애플리케이션을 수정하고 재배포하는 등의 작업이 발생하기 때문에 alias API를 통해 별칭을 설정해서 사용하는 것이 좋다.


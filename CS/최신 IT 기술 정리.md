# 목차

- [Big Data](#Big-Data)
- [블록체인](#블록체인)
- [인공지능](#인공지능)



# Big Data

# Clustering

- Clustering: 데이터를 유사도에 따라 k개의 그룹으로 나누는 것
  - 추천 시스템을 만드는데 주로 사용된다.



- Clustering이 잘 되었는지 판별하는 방법

  - n개의 데이터가 있을 때 모든 데이터를 Clustering하는 경우의 수는 2<sup>n</sup>개 이다.

  - Clustering을 할 수 있는 경우의 수는 데이터의 수에 따라 증가하고 빅데이터의 경우 셀 수 없을 만큼 만은 경우의 수가 존재하게 된다. 따라서 어떤 Clustering결과가 가장 유의미한지 판별이 가능해야 한다.

  - 판별 방법에는 여러 가지가 있는데 아래의 수식은 그 중 하나이다.
    $$
    \sum_{m=1}^k\sum_{t_{mi}\in{K_m}}^N (C_m-t_{mi})^2
    $$

    - Clustering한 그룹 내의 모든 포인터의 중심과 모든 그룹 내의 개별 포인터의 차이를 제곱해서 더했을 때 그 값이 작을 수록 Clustering이 더 잘됐다고 볼 수 있다.
    - 예를 들어 (1,1), (1,2), (2,1), (5,5), (5,6), (6,5), (7,7)의 7개의 포인터가 있고 이를  A: (1,1), (1,2), (7,7)  / B:  (2,1), (5,5), (5,6), (6,5)로 나누었을 때 A의 중심좌표는 (9/3,10/3) 즉, (3, 3.3)이고 B의 중심 좌표는 (18/4,17/4) 즉, (4.5, 4.2)이다. 이 때 A의 중심좌표인 (3, 3.3)에서 A의 개별 좌표들을 빼서 제곱한 후 모두 더하고, B의 중심좌표인 (4.5, 4.2)에서 B의 개별 좌표들을 빼서 제곱한 후 모두 더하면 결과값(예시의 경우 68.42)이 나온다.
    - A그룹만 예시로 계산을 해보면 A그룹의 중심 좌표는 (9/3, 10/3)이므로 (1-9/3)<sup>2</sup>+(1-10/3)<sup>2</sup>+(1-9/3)<sup>2</sup>+(2-10/3)<sup>2</sup>+(7-9/3)<sup>2</sup>+(7-10/3)<sup>2</sup>을 하면 결과값은 23.75가 나온다.
    - 반면에 A:(1,1), (1,2), (2,1)  /  B:(5,5), (5,6), (6,5),(7,7)로 나눌 경우 6.83이 나오게 된다. 평면도 상에 좌표를 찍어보면 훨씬 Clustering이 잘 되었다는 것을 눈으로 확인할 수 있다.
    - 이번에도 A그룹만 계산을 해보면 A그룹의 중심 좌표는 (4/3,4/3)이므로 (1-4/3)<sup>2</sup>+(1-4/3)<sup>2</sup>+(1-4/3)<sup>2</sup>+(2-4/3)<sup>2</sup>+(2-4/3)<sup>2</sup>+(1-4/3)<sup>2</sup>을 하면 결과값은 5.5가 나온다. 
    - A그룹의 계산 결과만 봤을 때 (1,1), (1,2), (7,7)로 Clustering했을 때의 결과값인 23.75보다 (1,1), (1,2), (2,1)의 결과값인 5.5가 더 작으므로 A그룹만 봤을 때는 클러스트링이 더 잘 되었다고 볼 수 있다(이 경우에는 B그룹도 값이 더 적게 나와 Clustering이 더 잘된 것이 사실이지만 한 그룹의 결과값이 낮다고 해서 전체적으로 Clustering이 더 잘 되었다고 볼 수 있는 것은 아니다)



## Partitional Clustering Algorithms

- 가능한 Clustering의 결과를 열거해보면서 최적화된 결과를 찾아내는 방식



- 실질적으로 방대한 데이터를 전부 뒤지는 것은 불가능하기 때문에 일부만 뒤지는 방식이 사용된다.



- K-Means Clustering Algorithm

  - 방식
    - 랜덤하게 clustering하여 k개 그룹으로 나눈다.
    - 각 그룹의 평균점(mean)을 계산 한다. 
    - 모든 그룹의 개별 포인터 들이 어떤 그룹의 평균점에 가까운지를 계산한다.
    - 더 평균점이 가까운 그룹으로 재할당한다. 
    - 재할당 후 다시 위의 과정을 반복한다.
    - 재할당될 포인터가 없으면 종료된다.

  - 문제점
    - 데이터의 사이즈가 지나치게 작거나 지나치게 크면 사용이 어렵다.
    - 평균점을 기준으로 데이터가 원형으로 분포하는 종류의 데이터일 때에만 사용이 가능하다.
    - outlier가 있을 경우 평균점 계산이 잘못될 수 있다.
  - k-Medoids
    - K-Means Algorithms의 변형
    - k-means 알고리즘이 포인터들의 평균점을 계산하여 가상의 평균점을 중심으로 데이터를 묶는 방식이라면 k-Medoids는 가상의 평균점이 아닌 실제 존재하는 하나의 포인터를 중심점, 혹은 평균점으로 잡고 이 포인터를 중심으로 데이터를 묶는 방식이다.
    - 일반적으로 포인터들이 모여 있는 곳에 있는 포인터 중에서 중심점이 될 포인터를 선택하기 때문에 outlier의 영향을 적게 받는다.



- Hierarchical Clustering

  - 방식(bottom-up 방식)

    - n개의 모든 개별 포인터들이 독립적인 상태로 시작(즉 n개의 클러스터가 존재)
    - 각 개별 포인터와 다른 모든 포인터의 거리를 계산
    - 가장 가까운 거리에 있는 포인터를 병합(한 번 할 때마다 클러스터의 수가 감소), 동일할 경우 가장 가까운 포인터중 아무 포인터나 선택
    - 원하는 클러스터의 수(k)가 될 때 까지 이를 반복

  - 몇 개의 포인터를 가진 서로 다른 클러스터 사이의 거리를 계산하는 방식은 몇 가지가 존재한다. 어떤 방식을 채택하는가에 따라 결과가 달라질 수 있다.

    - Single-link Algorithm(d<sub>min</sub>): 한 클러스터의 모든 포인터들과 다른 클러스터의 모든 포인터들의 거리를 전부 계산한 후 가장 짧은 거리를 거리로 간주.

      ![](기초.assets\min.png)

    - Mean-link(d<sub>mean</sub>): 두 클러스터를 병합한 후 병합된 클러스터 내부의 모든 개별 포인터의 거리를 계산하여 그 평균을 거리로 간주

      ![](기초.assets\mean.png)

    - Average-linkd(d<sub>avg</sub>): 한 클러스터의 모든 포인터들과 다른 클러스터의 모든 포인터들의 거리를 전부 계산한 후 그 평균값을 거리로 간주

      ![](기초.assets\average.png)

    - Complete-link Algorithm(d<sub>max</sub>): 한 클러스터의 모든 포인터들과 다른 클러스터의 모든 포인터들의 거리를 전부 계산한 후 가장 긴 거리를 거리로 간주

      ![](기초.assets\max.png)

    - Centroid-link: 각 클러스터의 중심값 사이의 거리를 거리로 간주(빨간 점 I, J는 가상의 평균점)

      ![](기초.assets\centroid.png)



## Density-Based Clustering Algorithms

- DBSCAN Clustering Algorithm
  - 용어
    - Eps: ε(epsilon), 특정 점에서부터의 거리
    - MinPts: Eps 내에 있는 점들의 수
    - Core point: 점 p의 Eps 내에 자기 자신을 포함하여 Minpts 이상의 점이 있으면 점 p는 Core point라고 할 수 있다.
    - neighbourhood: core point인 점 p의 Eps 내에 속하는 점들, neighbourhood이면서 core point일 수도 있으며 neighbourhood이면서 core point가 아닌 점을 border point라 부른다.
    - Border point: Core point가 되지는 못하지만 한 Core point의 Eps내에 있는 점을 Border point라 부른다.
    - Directly-density-reachable:  core point인 p와 p에 속한(neighbourhood인) q는 p로부터 directly-density-reachable하다고 할 수 있다.
    - Density-reachable: core point인 p의  neighbourhood이면서 core point인 r이 있고 q가 r의 neighbourhood라면 p로부터 q 는 density-reachable하다고 할 수 있다(단 p는 q의 neighbourhood가 아니어야 한다).
    - Density-connected: 점 p, q가 있을 때 또 다른 점 o가 있어서 o로부터 p로도 density-reachable하고 q로도 density-reachable하면 density-connected라고 부른다.
    - outlier: core point가 아니면서 다른 core point의 neighbourhood도 아닌(border point도 아닌)점을 outlier라 부른다.
  - 파라미터로 Eps와 MinPts를 필요로 한다.
    - density를 결정하는 파라미터
  - DBSCAN Clustering Algorithm의 clustering 결과는 두 프로퍼티를 만족해야 한다.
    - Maximality: data에 속한 임의의 쌍 Pi, Pj가 있을때 Pi가 결과상의 임의의 cluseter C에 속하면서 Pj가 Pi로부터 density-reachable하다면 Pj도 Pi와 같은 cluster인 C에 속해야 한다.
    - Connectivity: 만일 Pi, Pj가 같은 cluster인 C에 속한다면 Pi와 Pj는 density-connected여야 한다.
  - 방식
    - 하나의 core point를 찾는다.
    - 해당 core point의 neighbourhood들을 탐색하며 core point인 것을 찾는다.
    - 또 다른 core point가 있을 경우 위 과정을 반복한다.
    - border point만 남았을 경우 중단하며 하나의 cluseter가 완성된다.



## EM Clustering

- 추천 시스템을 구현하는 가장 기초적인 Clustering



- Generative model(생성 모델)
  - 데이터를 무선적으로 생성하는 모델이 존재한다고 가정, 모델은 사람들이 알 수 없으며 데이터를 기반으로 모델(파라미터)을 추측해야 한다. 모집단 추정과 유사
  - 예시
    - 알려진 파라미터: A와 B의 2개의 주머니가 존재한다, 주머니 안에는 빨간색 공과 파란색 공이 존재한다. 한 주머니 당 공은 4개씩 들어 있다.
    - 데이터: 총 9개의 공을 꺼낸 결과 빨간 공 8개와 파란 공 1개가 선택되었다.
    - 데이터와 알려진 파라미터를 기반으로 추측해야 하는 파라미터: A와 B 주머니를 선택할 확률, 한 주머니의 빨간색 공과 파란색 공의 비율
    - `모델1`: A 주머니에는 빨간 공 4개가, B 주머니에는 빨간 공 3개와 파란 공 1개가 있을 것이고 이 때 주머니 선택 단계에서 A 주머니를 선택할 확률이 B 주머니를 선택할 확률보다 2배 높을 것이다.
    - A주머니가 선택될 확률이 B 주머니보다 2배 높기 때문에 A주머니가 선택될 확률은 2/3, B 주머니가 선택될 확률은 1/3이다.
    - A주머니에서 빨간색 공이 나올 확률은 1, B주머니에서 빨간색 공이 나올 확률은 3/4이다.
    - A주머니에서 파란색 공이 나올 확률은 0, B주머니에서 파란색 공이 나올 확률은 1/4이다.
    - 따라서 빨간색 공이 나올 확률은 11/12, 파란색 공이 나올 확률은 1/12다.
    - 9개의 공을 꺼내서  빨간 공 8개와 파란 공 1개가 선택될 확률은 모델1을 기반으로 추측했을 때  (11/12)<sup>8</sup>*(1/12)=11<sup>8</sup>/12<sup>9</sup>
    - 모델2: A 주머니에는 빨간 공 4개가, B 주머니에는 빨간 공 3개와 파란 공 1개가 있다고 할 때 주머니를 선택 후 주머니 안에 있는 공을 선택하게 된다. 이 때 주머니 선택 단계에서 A 주머니를 선택할 확률이 B 주머니를 선택할 확률보다 2배 높다.
    - `모델2`: A주머니에는 빨간 공과 파란 공이 각기 2개씩, B주머니에는 빨간공이 3개, 파란공이 1개 들어있으며 이 때 주머니 선택 단계에서 A 주머니를 선택할 확률이 B 주머니를 선택할 확률보다 2배 높을 것이다.
    - 위와 동일한 방식으로 계산했을 때 빨간 공 8개와 파란 공 1개라는 데이터가 얻어질 확률은 5*7<sup>8</sup>/12<sup>9</sup>로 모델 1보다 낮은 확률이다.
    - 따라서 두 모델 중 `모델1`의 확률(likelihood)이 더 높으므로 `모델1`이 더 유망한(promising) 모델이라고 할 수 있다.



- EM Algorithm
  - likelihood를 최대화하는 파라미터를 찾기 위한 알고리즘
  - 데이터를 기반으로 cluster를 추측한다. 



## Probabilistic Latent Semantic Indexing(PLSI)

- EM Clustering보다 보다 복잡한 방식
  - A라는 사람이 글을 쓴다고 가정했을 때 이 사람이 어떤 글감을 선택하여 글을 쓸지를 확률로 나타낼 수 있다. 어떤 글감을 선택하느냐에 따라 글에 어떤 단어가 들어갈지에 대한 확률도 변화한다.
  - 여기서 A가 글에서 사용한 단어들은 관찰 가능한 데이터들이지만 어떤 주제를 선택할지, 어떤 단어를 선택할지는 관찰 불가능한 확률이다. 
  - 글에서 어떤 단어가 나올 때 마다 해당 단어가 나올 확률은 올라가게 되는 것이 되고 해당 단어가 많이 나오는 주제를 유추할 수 있게 된다. 사용된 단어를 통해 주제를 유추하는 것처럼 데이터를 통해 cluseter를 유추하는 방식이다.
- 예시
  - d1, d2 라는 2가지 문서가 있고 t1,t2라는 2가지 주제가 있으며 apple,banana라는 2가지 단어가 있을 다고 가정
  - d1에는 apple이라는 단어가, d2에는 banana라는 단어가 한 번은 들어있다는 사실을 알고 있음
  - 확률을 아래와 같이 무선적으로 할당(확률이므로 합은 반드시 1이어야 한다)
    - p(apple|t1)=0.6, p(banana|t2)=0.4
    - p(apple|t1)=0.3, p(banana|t2)=0.7
    - p(t1|d1)=0.6, p(t2|d1) =0.4
    - p(t1|d2)=0.3, p(t2|d2)=0.7
  - 이후 무선적으로 할당한 확률을 기반으로 계산(계산식은 추후 추가)







# Recommendation Systems

- Content based filtering method

  - item이나 product 등과 같은 actual content를 이용한다.
  - 각 item 간의 similarity를 이용하여 추천한다.

- Collaborative filtering method

  - 각각의 유저는 비슷한 다른 유저와 동일하게 행동한다는 가정, 다른 유저들이 추천에 영향을 미친다.

    - 다른 유저들의 의견을 이용, usage또는 preference pattern을 이용한다.

  - 유저가 직접 점수를 매긴 item들에 대한 rating을 이용해서 추천

  - Memory based method

    - 모델을 만드는 것이 아니라 여러 유저가 입력해온 데이터를 활용하는 방식

    - user-user similarity: 유저사이의 유사성을 파악, emory based method의 한 종류

      |      | p1   | p2   | p3   | p4   |
      | ---- | ---- | ---- | ---- | ---- |
      | c1   | 0    | 1    | 0    | 1    |
      | c2   | 0    | 1    | 1    | 1    |
      | c3   | 1    | 0    | 1    | 0    |

      |      | c1   | c2   | c3   |
      | ---- | ---- | ---- | ---- |
      | c1   | 1.00 | 0.82 | 0.00 |
      | c2   | 0.82 | 1.00 | 0.20 |
      | c3   | 0.00 | 0.20 | 1.00 |

      - c1-c2는 유사도가 높고, c1-c3는 유사도가 낮다.

      - c1과 가장 유사도가 높은 c2가 구매한 물건 중 아직 c1이 구매하지 않은 물건을 c1에게 추천해주게 된다. 아래 표는 위 두 표를 기반으로 생성한 표로 c1은 이미 구매한 p2,p4를 제외하고 가장 값이 큰 p3를 추천 받게 될 것이다.

        |      | p1   | p2   | p3   | p4   |
        | ---- | ---- | ---- | ---- | ---- |
        | c1   | 0.00 | 1.82 | 0.82 | 1.82 |
        | c2   | 0.20 | 1.82 | 1.20 | 1.82 |
        | c3   | 1.00 | 0.20 | 1.20 | 0.20 |

      

  - Model based methods

    - 유저 데이터베이스를 사용하여 모델을 생성하는 방식 

    - Matrix Factorization: Model based methods의 한 방식, 주로 영화 추천에 사용, 아래 표에서 행은 유저를 나타내고, 열은 영화를 나타내며 숫자는 영화에 매긴 평점을 나타낸다. 비어있는 칸은 유저가 아직 평점을 매기지 않은 영화이다.

      |      | m1   | m2   | m3   | m4   |
      | ---- | ---- | ---- | ---- | ---- |
      | u1   | 5    |      |      | 3    |
      | u2   | 4    |      | 5    | 2    |
      | u3   |      |      | 3    | 1    |
      | u4   | 4    | 4    |      |      |

      - 행의 유저 정보와 열의 영화 정보를 분리한후 유저가 기존에 입력한 평점을 기반으로 현 유저와 가장 유사한 유저와 현재 평점을 매긴 영화와 가장 유사한 영화를 각기 찾은 뒤 다시 합친다.
      - 이 결과를 통해 아직 평가하지 않은 영화에 예상 평점을 보여준다.









# 블록체인

- 블록체인: 서로 신뢰 할 수 없는 환경에서 사람들이 중립적이고 중앙화된 인증기관 없이 신뢰를 보장하는 기술, 간단히 말해서 신뢰를 만드는 기계



## 기술적 배경

- P2P 네트워크를 이용하여 거래내역을 분산, 온라인 네트워크 참가자 모두에게 내용을 공개 및 기록
  - 새로운 거래가 일어날 때마다 노드들(참가자)이 가진 블록체인을 업데이트, 무결성을 유지하도록 하는 분산형 거래 시스템
  - 기존의 금융 시스템이 은행이 모든 거래 내역을 기록한다면 블록체인 시스템은 모든 참가자들이 거래 내용을 기록
    - 모든 참가자가 거래 내역을 지니고 있으므로 위조, 변조가 어렵다. 한 개인이 거래 내역을 위조한다고 하더라도 다른 참가자들이 지닌 내역과 비교하면 위조를 판별할 수 있다.
    - 일정 기간 동안 거래 내역을 저장하고 이상이 없는지 검사 후 노드들의 블록체인을 업데이트 하는 방식



- 퍼블릭 블록체인 VS 프라이빗 블록체인
  - 퍼블릭: 허가를 받지 않아도 누구나 참가가 가능한 블록체인
  - 프라이빗 블록체인: 허가를 받은 노드마 참가가 가능한 블록체인



## 블록체인 핵심 기술 구성도

- P2P 네트워크(P2P Netword): 모든 노드들이 정보(거래 내역)를 공유



- 암호화(Encryption): Hash라는 암호화 기술을 사용, 입력 값과 전혀 무관한 내용으로 변환 후 저장하여 입력값을 예측하기 어렵게 만든다.



- 파일 매니저(File Manager): block화 기술이라고도 할 수 있다. 일정 기간 동안 거래 내역을 저장하여 해당 내용을 block화, 이후 해당 block을 해쉬 함수를 통해 암호화하여 암호화된 결과값을 다음에 생성되는 블록에 저장한다. 즉 나중에 생성된 블록은 이전에 생성된 블록에 대한 정보를 가지고 있다.
  - 과정을 간단하게 예시로 살펴보면 다음과 같다.
    - 10분간의 거래 내역을 저장 후 block화
    - 이후 block을 해쉬 함수를 통해 암호화
    - 암호화된 내용을 다음 블록에 저장
    - 위의 과정을 10분 주기로 반복
  - 즉 모든 블록은 해쉬값으로 연결되어 있다.
    - 따라서 한 노드가 자신의 거래 내역을 위조하고자 한다면 해당 거래 내역이 저장된 모든 블록을 위조해야 한다.
    - 예를 들어 현재까지 200개의 블록이 생성되었고 100번째 블록에 저장된 거래 내역을 위조하고자 한다면 완전히 위조하기 위해서는 100번째 블록 뿐 아니라 100~200번째 블록의 내용을  모두 위조해야 한다.



- 합의 알고리즘(Consensus Algorithm)
  - POW(Proof Of Work)방식:비트코인, 이더리움 등이 사용하는 방식으로 거래 내역이 가장 많은, 일을 가장 많이 한 노드를 신뢰하고 해당 노드의 블록을 검사하고 전파하는 합의 방식, 모든 노드를 확인해야 한다는 부작용이 존재
  - 노드들의 지분을 가지고 투표를 통해 합의하는 방식



- 스마트 컨트랙트(smart contract)
  - 이더리움에 추가된 기술로 블록에 거래 내역과 같은 사실 관계 뿐 아니라 코드도 추가하여 프로그램이 블록 내에서 실행될 수 있도록 하는 기술
  - 계약에 의한 거래가 발생할 경우 계약 내용을 프로그램화 시켜 누구도 계약 내용을 위조할 수 없도록 하고 반드시 이행되도록 하는 것이 가능하다.
    - 예를 들어 A가 B에게 1비트코인을 선물 할 때 10시 이후에는 사용이 불가능하다는 것을 조건으로 선물했다면 해당 내용을 프로그램화하여 10시 이후에 사용이 불가능해지도록 할 수 있다.





## 블록체인 시장의 변화

- 가상화폐(암호화폐)
  - 현황
    - 비트코인, 라이트 코인, 리플, 이더리움 등이 존재
    - 국가의 고유한 권한처럼 여겨졌던 화폐를 기업이 발행하는 것이 가능해져 경제 민주화라고 까지 부르는 사람도 존재
    - 영국, 일본 등에서는 화폐로 인정했으나 한국, 중국 등은 법적으로 금지
  - 비즈니스 도메인 활성화 코인
    - 스팀잇의 스팀코인: 자신의 콘텐츠가 많은 공감을 받거나 많은 댓글을 받게 되면 스팀 파워가 올라가고 그 대가로 스팀 달러로 보상
    - 클라우드의 파일코인: 점점 더 큰 저장소가 필요해지면서 비어있는 저장소를 제공하면 그에 대한 인센티브로 파일코인을 제공, 클라우드 저장소의 중앙화 문제 해결
    - IoTA: 사물인터넷과 블록체인을 융합



- 비금융권 응용 확대
  - 인증/관리: 전자문서 관리, 인증 권한 관리, 디지털 신분증
  - 자산 거래: 부동산 거래 시장, 다이아몬드 인증/거래, 금/은 거래
  - 기타: 예측 시장, 온라인 투표, 개인 의료 정보, 디지털 콘텐츠 소유권, IoT
    - IoT와 블록체인이 융합한 예시로 Slock it이 있다.
      - Slock it에서 제작한 도어락이 설치된 방에서 숙박을 원하는 경우 여햊아가 사용 기간을 설정 후 비용을 지불하면 해당 방에 설치된 도어락을 제어, 사용 기간이 만료되면 자동으로 도어락에 대한 사용권을 반납
      - AirBnB 없이도 AirBnB와 같은 효과를 낼 수 있다.
    - Coldchain
      - 화물을 수송하는데에 드는 비용보다 해당 화물에 적합한 온도를 유지하는 비용이 더 많이 들어 일부 운송업자는 온도를 조작하기도 한다.
      - 온도를 조작 후 해당 정보를 저장하는 IoT 기업에 자신이 조작한 내역을 삭제해 달라는 요청이 빈번히 들어온다고 하는데 이를 블록체인으로 관리하면 조작이 불가능해진다.
      - 즉 해당 정보를 IoT회사가 아닌 블록체인에 기록되게 하여 조작할 수 없게 만드는 것이다.
    - 딥마인드 헬스
      - 정보 수집 과정에서 보안 논란이 일자 블록체인 기술을 도입
      - 데이터가 언제, 어떤 목적으로 사용되었는지에 대한 기록을 블록체인 상에 저장



- 공유경제 사회의 신뢰 구축을 위한 인프라로 각광

  - 공유경제: 한번 생산된 제품을 여럿이 공유해 사용하는 협업 소비 근간의 경제 방식

  - 공유 경제의 가장 큰 문제점은 제품을 공유하는 상대방을 신뢰할 수 없다는 것이다.
  - 공유경제에서 발생하는 모든 데이터를 블록체인에 기록 및 공유하면 데이터의 무결성과 거래의 신뢰성이 보장될 수 있다.

  - 에너지 또한 개인이 생산한 에너지를 기업에서 구매했는데 블록체인 기술이 활성화 되면 개인과 개인간 직접적인 신뢰로운 에너지 거래가 가능해진다.



- 블록체인에 뛰어드는 대기업의 5가지 유형
  - 코닥
    - 블록체인을 이용한 사진 저작권 정보 저장 및 사진 거래 플랫폼을 제공
    - 필름에서 디지털로 변화하던 시대에 적응 못해 파산했으나 다시 일어서기 위해
  - 토요타: 
    - 블록체인을 이용한 차량 공유 플랫폼
    - 우버 중심의 차량 공유 시스템을 흔들기 위해
  - 월마트
    - 농축산품 유통과정 저장
    - 농축산품 유통 시장을 독점하기 위해
  - 에어버스
    - 스마트 계약을 통한 각국 협력, 업체와의 거래 및 3D 설계도 유출 방지
    - 보다 저렴한 비용으로 서비스를 관리하기 위해서
  - 토쿄전력
    - 블록체인을 이용한 에너지 거래 플랫폼, 개인간 태양광 에너지 거래 가능
    - 뒤쳐질 것 같다는 불안감 때문에







# 인공지능

## 머신러닝과 다른 학문들과의 관계

- 머신러닝과 빅데이터: 머신러닝은 빅데이터를 분석하고 이해하고 예측하는 한 방식이다.

- 머신러닝과 데이터 마이닝의: 머신러닝은 정형화된 데이터를 필요로 하지 않지만 데이터 마이닝은 정형화된 데이터를 필요로 한다.

- 머신러닝과 인공지능: 머신러닝은 데이터에 기반하여 인공지능을 개발하는 방법이다.
- 머신러닝과 통계학 : 머신러닝은 통계학에 기반하지만 통계학에서 다루는 데이터보다 훨씬 더 많은 양의 데이터를 다루어 통계학의 한계를 극복한다.



## 머신러닝의 주요 주제

- 지도학습(supervised learning)
  - 정답을 가지고 있는 학습 데이터를 통해 학습
  - 예를 들어 여러 종류의 자동차(정답) 사진을 입력한 후 입력되지 않은 사진들 중 자동차 사진을 가려내게 하는 것은 지도학습이라고 할 수 있다.
  - 답이 있는 데이터일 경우에만 가능한 학습 방법
- 비지도학습(unsupervised learning)
  - 따로 정답 데이터를 입력하지 않고 다양한 데이터를 입력하여 알아서 학습
  - 꽃 사진과 자동차 사진을 입력한 후 꽃과 차를 구분하게 하는 것은 비지도 학습이라고 할 수 있다.
- 강화학습(reinforcement learning)
- representation learning
  - Neural Network의 발전에 따라 각광을 받기 시작
  - 기술의 발전에 따라 데이터의 사이즈는 점차 증가했다. 예를 들어 사진만 하더라도 화소가 점차 증가하면서 용량이 점차 커져갔다. 이는 대량의 데이터를 처리해야 할 때에 큰 불편을 초래했다. 데이터에서 의미 있는 정보만을 남기고 의미 없는 정보는 정리하여 사이즈를 줄이는 작업이 필요해졌는데 이를 가능하게 해주는 것이 Neural Network이다.
  - Neural Network을 사용하여 흑백 사진을 분석하는 과정을 예로 들면 
    - 가장 처음에 백~흑 까지의 색을 찾아내는 작업을 한다. 
    - 흑백 사진에서 흑색은 선을 표현하는데 사용되므로 흑색이면 직선, 혹은 곡선이 있다고 인식한다.
    - 이목구비는 각기 다른 형태의 해당 직선, 곡선들을 지니므로 이를 분류하여 이목구비를 찾아낸다.
    - 이목구비가 모두 있으면 얼굴로 인식한다.
  - Neural Network을 사용하지 않았을 때에는 흑백 찾기 알고리즘, 곡선과 직선을 구분하는 알고리즘, 이목구비를 구분하는 알고리즘, 이목구비를 찾는 알고리즘을 각기 개발해야 했다.



## Naive Bayes Classifier

- 컴퓨터는 사물을 분류할 때 성질(features)를 활용한다.
  - 예를 들어 얼굴의 성질은 이목구비가 있다는 것일 수 있다.
  - 그러나 컴퓨터는 이목구비가 있다는 것을 그냥 이해할 수는 없고 숫자로 변환을 해주어야 한다.
  - 이미지나 문서를 컴퓨터가 이해할 수 있는 숫자로 표현한 것을 features라고 한다.



- Naive Bayes Classifier:Naive Bayes Classifier는 공통적으로 모든 특성 값은 서로 독립임을 가정한다. 특성들 사이에서 발생할 수 있는 연관성이 없음을 가정하고 예를 들어, 특정 과일을 사과로 분류 가능하게 하는 특성들 (둥글다, 빨갛다, 지름 10cm)은 각각의 특성들이 특정 과일이 사과일 확률에 독립적으로 기여 하는 것으로 간주한다.
  - 각각의 독립적인 특성들이 분류하고자 하는 대상과 얼마나 유사한지를 계산하고 그 결과값을 통해 분류 대상이 특정 분류에 속할 확률을 나타내준다.







## Deep Natural Language Processing

- Word Embedding: 임의의 길이의 단어 하나를 Vector로 표현하는 것



- 각 단어들은 각각의 차원을 지니고 있으며 비슷한 의미이거나 연관성이 높은 단어는 가까운 차원에 존재한다.
  - 예를 들어 영국과 런던, 프랑스와 파리는 연관성이 높은 단어이므로 가까운 차원에 존재하게 된다.
  - 이는 Distributional Hypothesis에 근거한 것이다.
  - Distributional Hypothesis: 같은 맥락에서 나오는 단어는 유사한 의미를 지니고 있을 것이라는 가설



- Many-to-many relationship: 하나의 concept은 많은 neurons를 지니고 있고 하나의 neuron은 많은 concepts를 지니고 있다.